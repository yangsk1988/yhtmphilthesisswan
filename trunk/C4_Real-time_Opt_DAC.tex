
\section{Consensus Based Signal Processing}

Based on the FT-DCA algorithm we introduced in the last section, actually
each node can extract more information from local values sequence,
if some signal processing techniques are used. These signal processing
techniques introduced in the following may also be applied to wireless
sensor networks as they could be carried out distributively. 


\subsection{Consensus Based Network Information Flooding(todo check)}

The conventional information flooding actually is done by copy information
to all other nodes in the network. Each node maintains a table of
the values of all nodes in the network, initialized with its own node
value. And nodes exchanges the tables of their own and those from
their neighbors in each iteration. After a certain number of iterations
which is equal to the diameter of the network, each node will knows
value of all the nodes. It is also a method to implement distributed
averaging. 

However, copying information and forwarding to all other nodes takes
too much resources for the networks. If a networks has $n$ nodes,
the conventional flooding will needs at least $\left(n-1\right)$
copies for each piece of information. And this estimation doesn't
considering the cost in transmitting the information to the destination.
Therefore, in this section we propose a novel network information
flooding technique based on consensus algorithm. It doesn't require
copying information for many time, but transmit information by broadcasting.
The advantage of this method is that it can save the costs in copying
and transmitting information. 

Suppose the network weight matrix is $\mathbf{W}$ that satisfies
the same condition \ref{eq:convege condition W}. Recall the initial
local value decomposition given in Eq.\eqref{eq:initial vector decompose}

\begin{equation}
\mathbf{x}\left(0\right)=\sum_{j=1}^{n}\alpha_{j}\mathbf{e}_{j}\label{eq:initial vector decompose}
\end{equation}
which shows that the initial local value vector can be defined by
a set of coefficients and a new basis defined by the eigenvectors.\textbf{
}In addition, the eigenvectors $\mathbf{e}_{i}$ are only depends
on the weight matrix. Therefore, once the coefficients $\alpha_{i}$
and weight matrix are available, the initial values vector $\mathbf{x}\left(0\right)$
can be calculated. It is possible to exchange information between
nodes in the network without copying information to all other nodes.

To show how this method can be carried out distributively, recall
the FT-DCA algorithm in section \eqref{sub:Find-the-Consensus}. Let
the sample vector $\mathbf{y}_{i}(k,d)\in R^{d}$ is defined by the
history of $x_{i}(k),$ 

\begin{equation}
\mathbf{y}_{i}(k,d)=\left[x_{i}(k),x_{i}(k-1),\ldots x_{i}(k-d+1)\right]^{\mathrm{T}}
\end{equation}
and the coefficients vector $\mathbf{a}=\left[\alpha_{1},\alpha_{2},\ldots,\alpha_{n}\right]^{T}$.
If we involve the eigenvectors and redefine the \prettyref{eq:Vander matrix and consensus-d*m}
by 
\begin{equation}
\mathbf{y}_{i}(k,d)=\mathbf{V}_{i}(k,d)diag\left(\left[\mathbf{e}_{1}^{T}\mathbf{u}_{i},\mathbf{e}_{2}^{T}\mathbf{u}_{i},\ldots,\mathbf{e}_{n}^{T}\mathbf{u}_{i}\right]\right)\mathbf{a}\label{eq:Vander*Eigen i^th*Alpha}
\end{equation}
where $\mathbf{u}_{i}$ is the unit vector with all zero except the
$i^{th}$ component is one, $\mathbf{e}_{j}^{T}\mathbf{u}_{i}$ means
the $i^{th}$ component of $\mathbf{e}_{j}$. Solving the above equation
will obtain the coefficients $\alpha_{j}$. 

The consensus based information flooding is ideally suitable for time-invariant
network. Because a node must know all the eigenvectors and eigenvalues
of the network before it can estimate initial values of other nodes.
This requires each node flooding its weight coefficients to all nodes
in the network at first. However, instead of flooding the table of
initial values, flooding the weight matrix is only performed at the
stage of network initialization or when the network topology changes.
If the frequency of topology is very low or in a range that acceptable,
the proposed method could have lower cost both in computation and
communication.


\subsection{Consensus Based Decentralized Eigenvalues Estimation(todo)}

(todo compare existing algorithm AESOPS AND PEALS ALGORITHMS in power
system)

In ad-hoc network, each node has the knowledge of network topology
is a strong condition. Although nodes in the network can flood the
table of their adjacent nodes to build up network topology, it is
still a problem to maintain this table when the network topology changes
frequently. 

However, in the distributed signal processing and multi-vehicle cooperative
control \cite{Fax2004}, the knowledge about network topology is sometimes
very important. In these applications, the distributed consensus algorithm
is widely used and requires knowledge of the graph matrix to optimize
the convergence rate. For example, \cite{Xiao2004} proposed a method
to find the optimal matrix for FO-DCA which requires the whole network
topology. Even thought higher-order distributed average consensus
algorithm doesn't have so strong requirement, but the eigenvalues
of the graph matrix or Laplacian matrix are required \cite{Xiong2010}.
The paper \cite{Chung2006} introduce a method to estimate the range
of eigenvalues of Laplacian matrix. However, this method is still
questionable in application due to the following reasons. First, the
range is not accurate enough to satisfy the requirement of convergence
rate optimization. Second, it requires many communication resources
in the transmission of node degree and computation of minimum spanning
tree. Third, it cannot estimate the range for all the eigenvalues,
as higher-order DAC algorithm need to use more eigenvalues.

Some sophisticated algorithm, such as the finite-time consensus \cite{Sundaram2007}
and adaptive filter algorithm for consensus \cite{Cavalcante2010},
can converge to the consensus value using an adaptive filter or even
find the consensus value in finite number of iterations. However,
it has been proved that the consensus value can be a linear combination
of local values obtained by FO-DCA. Thus, the accuracy of the estimated
consensus value highly relies on the accuracy of coefficients multiplying
these local values. The reliability to network changes of these methods
is still questionable. In addition, all these methods have an instance
of FO-DCA running in background. Therefore, it is very hard to say
that the asymptotic consensus algorithms are out of time. In contract,
they are already applied to network with a large number of nodes and
robust to the topology variation. 

If the network topology and all the weights to edges are known to
some centralized nodes in the network, then the eigenvalues can be
calculated and flooded to all nodes, which could use these values
to optimize the convergence rate of DCA or even achieve distributed
consensus in finite-time. However, in some cases there is no such
a centralized node to collect all the information and doing the calculation
and transmission of parameters. Therefore, it is necessary for nodes
to estimate these parameters with local information they can access.

The work of this section is as follows, first we analyze the properties
of local value iteration and proposed an algorithm to estimate these
necessary eigenvalues based on FO-DCA. Second, we try to use the estimated
eigenvalue to increase the convergence rate of the original asymptotic
consensus algorithm. 

Suppose we have a distributive network with the same system model
described in \ref{sec:Consensus-problem-on}. The weight matrix satisfies
the condition Eq.\ref{eq:convege condition W}. Then, the minimal
polynomial of the weight matrix an unique eigenvalue equal to one 

\[
p(\lambda)=\prod_{i=1}^{m}\left(\lambda-\lambda_{i}\right)=\lambda^{m}+a_{m}\lambda^{m-1}+\ldots+a_{2}\lambda+a_{1}
\]
By letting the $p(\lambda)=0$ and finding the solution, $\lambda_{i}$
are the eigenvalues to be estimated. Therefore, the eigenvalues estimation
problem is equivalent to the problem of finding the coefficients $\left\{ a_{m}\right\} $. 

Since coefficients $\left\{ a_{m}\right\} $ also lead to a linear
combination expression given in \ref{eq:local value linear predictor}.
They can be estimated by the inverse Toeplitz matrix and solving Eq.\ref{eq:Toepliz Eq.}. 

However, when network contains more nodes and the matrix size becomes
larger, the matrix is almost singular or loss rank so that the numerical
error of the coefficients becomes larger. Even though sometimes it
may be accurate enough to estimate the consensus value with high accuracy,
it is unacceptable for eigenvalues estimation. This is because $p(\lambda)$
is a high order polynomial which is very sensitive to the numerical
error of its coefficient. Thus, the variation of the solutions is
much larger than numerical error of these coefficients. 

(todo if we need to show the numerical error, when solving this problem
in double precision. There can be ``iterative improvement of a solution''
to mitigate the numerical error. )

Take a deep inspection of the Toeplitz matrix when it becomes large,
one of the several reasons for the numerical problem is because the
local value $x_{i}\left(k\right)$ approaches the global average asymptotically,
as the it has $\left\Vert \mathbf{x}\left(k+1\right)-\mathbf{\bar{x}}\right\Vert <\left\Vert \mathbf{x}\left(k\right)-\mathbf{\bar{x}}\right\Vert $.
Thus, the corresponding row in Toeplitz matrix have entries with very
little difference.  

To mitigate the numerical problems, the Toeplitz matrix should be
constructed by some other way. Not only by the local values obtained
from one instance of FO-DCA, but also from other FO-DCA iterations.
When the old local values vector is getting too close to the consensus
values. the FO-DCA is reinitialized with a new local value vector
which is preferable to be independent of the old one.

Let $\mathbf{x}_{1}\left(0\right),\mathbf{x}_{2}\left(0\right),\ldots,\mathbf{x}_{N}\left(0\right)$
denote $N$ different and independent initial local value vectors.
For each initial vector the FO-DCA take at least $2D+1$ iterations,
where $D+1$ is the width of Toeplitz matrix. Due to the property
of minimal polynomial of weight matrix, the $D$ should be larger
or equal to the number of distinct and nonzero eigenvalues of the
weight matrix $\left(D\geq m\right)$. After this initialization,
each node $i$ will have the history of local values obtained by the
$N$ instance of FO-DCA. Therefore the new Toeplitz matrix can be
constructed by

\[
T=\left[\begin{array}{c}
T_{i,1}\\
T_{i,2}\\
\\
T_{i,N}
\end{array}\right],\mbox{where }T_{i,j}=\left[\begin{array}{cccc}
x_{i,j}\left(D\right) & x_{i,j}\left(D-1\right) & \ldots & x_{i,j}\left(0\right)\\
x_{i,j}\left(D+1\right) & x_{i,j}\left(D\right) & \cdots & x_{i,j}\left(1\right)\\
\vdots & \vdots & \ddots & \vdots\\
x_{i,j}\left(2D\right) & x_{i,j}\left(2D-1\right) & \cdots & x_{i,j}\left(D\right)
\end{array}\right]
\]
Similarly to Eq.\ref{eq:Toepliz Eq.}, the vector $\mathbf{y}=\left[\mathbf{y}_{i,1}^{T},\mathbf{y}_{i,2}^{T},\ldots,\mathbf{y}_{i,j}^{T}\right]^{T}$
is constructed by the corresponding local values, where $\mathbf{y}_{i,j}=\left[x_{i,j}\left(D+1\right),x_{i,j}\left(D+2\right),\dots,x_{i,j}\left(2D+1\right)\right]^{T}.$
Thus, we have the new equation give by 
\begin{equation}
\mathbf{y}=T\mathbf{a}\label{eq:expanded Toeplitz Eq.}
\end{equation}
Solving the above equation will involve the Moore-Penrose pseudo-inverse
\cite{Piziak2007}. it worth noting that Moore-Penrose pseudo-inverse
of $T$ actually find the least mean square solution. If we take more
initial local value vectors and run more instances of FO-DCA, the
numerical solution of this equation could be more close to the real
solution. 


\subsection{Consensus based Data Fusion and Decision Making}

Sensor networks have a variety of applications in surveillance and
environment monitoring, data gathering from spatially distributed
sources, collaborative signal processing. A fundamental problem in
sensor network is to process the local acquired data or signal using
a scalable algorithm \cite{Olfati-Saber2005a}. 

Generally, there are two options for multiple sensors signal processing:
First option is centralized signal processing. This requires the network
contains a fusion center, and all sensor's information being transmuted
to the central processor where the global LLR is calculated. At the
same time, a hypothesis test based on the ML, MAP or Bayesian decision
rule will be carried out at the a fusion center to make the decision.
Besides, an optimal data fusion scheme is proposed in \cite{Chair1986},
the decision is made by an optimal linear combination of local decisions
of all sensors. 

The Second option is distributive signal processing. In the consideration
of reliability, survivability, and increase in range of coverage,
there is an increasing interest in employing multiple sensors for
these applications \cite{Chair1986}. Then, global LLR should be calculated
in a distributive manner. In this section, we will consider a distributed
detection problem in wireless sensor network without the fusion center.
Then, there will be an introduction about consensus based approach
in the distributed data fusion and decision making, in the case of
each sensor acquires a scale value of an unknown parameter. However,
\cite{Xiao2005} discussed the case when each sensor acquires a vector
of unknown parameters and the signal is mixed with joint Gaussian
white noise, and proposed a more sophisticated data fusion scheme. 

(todo model: multiple sensors, data fusion.)

Considering a binary hypothesis testing problem with the following
two hypotheses
\begin{enumerate}
\item H0: target is absent
\item H1: target is present.
\end{enumerate}
Suppose each sensor has the following observation

\begin{equation}
x_{l}=\begin{cases}
\mu_{l,0}+n_{l} & \mbox{if no target presents}\\
\mu_{l,1}+n_{l} & \mbox{if target presents}
\end{cases}
\end{equation}
where $\mu_{l,m}$ is the mean value of $x_{l}$ depending on hypothesis
$m$ and $n_{l}$ is the noise of $x_{l}$ . we need to make a declaration
of the target presentation or absence based on sensors local observations.
The prior probability of these two hypotheses is denoted by $P\left(H_{m}\right)=P_{m},m=1,2$.

Based on classical hypothesis test theory, the global log likelihood
ratio (G-LLR ) test is given by 

\begin{equation}
LLR(x_{1},...,x_{L})=\log\frac{f\left(x_{1},...,x_{L}|H_{1}\right)}{f\left(x_{1},...,x_{L}|H_{0}\right)}\underset{H_{0}}{\overset{H_{1}}{\gtrless}\log}\frac{P\left(H_{o}\right)}{P\left(H_{1}\right)}\label{eq:G-LLR define}
\end{equation}
where $f\left(x_{1},...,x_{L}|H_{m}\right)$ is the likelihood function
of hypothesis $H_{m}$.

To calculate the LLR in a distributed manner, a usual simplification
is assuming sensor observations are independent from one to another.
Therefore, we have $f\left(x_{1},...,x_{L}|H_{m}\right)=f\left(x_{1}|H_{1}\right)\cdot\ldots\cdot f\left(x_{L}|H_{1}\right)$,
and 

\begin{eqnarray}
LLR(\mathbf{x}) & = & \log\frac{f\left(x_{1}|H_{1}\right)\cdot\ldots\cdot f\left(x_{L}|H_{1}\right)}{f\left(x_{1}|H_{0}\right)\cdot\ldots\cdot f\left(x_{L}|H_{0}\right)}=\sum_{i=1}^{L}LLR\left(x_{i}\right)\label{eq:Sum L-LLR}
\end{eqnarray}
. As seen from the above equation, in another option of data processing,
if the sensor node could calculate its local log likelihood ratio
instead of the complete local observation $x_{i}$ , the G-LLR changes
into the sum of local log likelihood ratio (L-LLR). 

According to Eq.\ref{eq:Sum L-LLR Cloud}, the G-LLR is equal to the
average multiply with the number of sensors in the network. Therefore,
it can be calculated by distributed average consensus (DAC) algorithm.
This is implemented by the following steps: First, each sensor calculates
its local LLR individually; Then, all the sensors update their local
LLR in the DAC iteration until they converge to a common value; Finally,
once the algorithm converges, the global LLR is obtained by multiply
the average local LLRs with the number of sensors in the network. 

In the next, we will generalize the conditions step by step and drive
some expressions to show the possibility of calculating G-LLR with
DAC algorithm. First we assume the sensors observation is corrupted
by joint Gaussian white noise $\mathbf{n}\sim\mathcal{N}\left(0,\Sigma\right)$,
which means noises are not independent from one sensor to another.
Based on this assumption, the expression of G-LLR evolves into a weighted
sum of sensors observation. In section \prettyref{sub:Distributed-Cloud-Declaration},
we will take a further step. The sensor observations will be mixed
with joint Gaussian white noises which is not only dependent among
other sensors but also depends on the target existence. More results
about the applying DAC algorithm in decision making will be presented. 

Let the sensors observation, the joint Gaussian white noises and the
mean of sensors observation in a vector form given by 
\[
\mathbf{x}=\left[x_{1},\ldots,x_{L}\right]^{\mathrm{T}}
\]


\[
\mathbf{n}=\left[n_{1},\ldots,n_{L}\right]^{\mathrm{T}}
\]


\begin{equation}
\mathbf{u}_{m}=\left[\mu_{1,m},\ldots,\mu_{L,m}\right]^{\mathrm{T}},\; m=0,1.
\end{equation}
Since $\mathbf{n}\sim\mathcal{N}\left(0,\Sigma\right)$, the likelihood
function is to be joint Gaussian function $f\left(x_{1},...,x_{L}|H_{m}\right)=\frac{1}{\left(2\pi\right)^{L/2}\left|\Sigma\right|^{\frac{1}{2}}}\exp\left(-\frac{1}{2}\left(\mathbf{x}-\mathbf{u}_{m}\right)^{T}\Sigma^{-1}\left(\mathbf{x}-\mathbf{u}_{m}\right)\right)$,
the G-LLR becomes, 
\begin{eqnarray}
LLR(\mathbf{x}) & = & \left(\mathbf{u}_{1}^{\mathrm{T}}-\mathbf{u}_{0}^{\mathrm{T}}\right)\mathbf{\Sigma}^{-1}\mathbf{x}+\frac{1}{2}\left(\mathbf{u}_{0}^{\mathrm{T}}\mathbf{\Sigma}^{-1}\mathbf{u}_{0}-\mathbf{u}_{1}^{\mathrm{T}}\mathbf{\Sigma}^{-1}\mathbf{u}_{1}\right)\label{eq:wight sum of detection}\\
 & = & \sum_{l=1}^{L}w_{l}x_{l}+C\nonumber 
\end{eqnarray}
where $w_{l}$ is the $l^{th}$ component of $\left(\mathbf{u}_{1}^{\mathrm{T}}-\mathbf{u}_{0}^{\mathrm{T}}\right)\mathbf{\Sigma}^{-1}$,
$C$ is the last term the equation. This equation means the LLR can
be a weighted sum of the signal acquired in each sensor.

Provided that each sensor knows the weight $w_{l}$, the Eq.\ref{eq:wight sum of detection}
states that the G-LLR is equal to the weighted sum of local observation
of sensors in the network together with the constant $C$. Actually,
the constant $C$ changes the threshold of the hypothesis testing
and can be subtract from both side of hypothesis testing equation.
Therefore, we modify the hypothesis testing into 
\[
\sum_{l=1}^{L}w_{l}x_{l}\underset{H_{0}}{\overset{H_{1}}{\gtrless}}\frac{P\left(H_{o}\right)}{P\left(H_{1}\right)}-C
\]
where an average of the value $w_{l}x_{l}$ is obtained by distributed
average consensus (DAC) algorithm. Then we multiply the average with
the number of sensors in the network to get $\sum_{l=1}^{L}w_{l}x_{l}$.
This finish the distributed detection and decision making given the
noise are jointly white Gaussian. More generalized condition of calculating
$LLR(\mathbf{x})$ when sensor signals are correlated is discussed
in \prettyref{sub:Distributed-Cloud-Declaration}. 
