#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass book
\options onecolumn,12pt,A4paper
\use_default_options true
\begin_modules
eqs-within-sections
theorems-ams-bytype
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman cmr
\font_sans default
\font_typewriter default
\font_default_family rmdefault
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing onehalf
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 4cm
\topmargin 3cm
\rightmargin 2.5cm
\bottommargin 3cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes true
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\author 5862369 "HT" 
\author 5863457 "ht" 
\author 936221351 "LiminYao" 
\end_header

\begin_body

\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "chap:Online-Optimization-of"

\end_inset

Real-time Optimization of DAC 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Distributed average consensus (DAC) algorithms are widely used in many applicati
ons.
 It utilizes matrix iteration to find the dominant eigenvector.
 To minimize the required number of iterations, the algorithms need to be
 optimized.
 However, this optimization needs the knowledge of network topology, which
 is very hard to obtain for an individual agent in distributed networks.
  Thus, optimal step length and forgetting factor need to be calculated
 offline and forwarded to every agent.
  To solve this problem, we propose
\change_deleted 5863457 1357427850
d
\change_unchanged
 a distributed real-time optimization technique so that each node can estimate
 these optimal parameters individually.
 In addition, the method is based on constant first-order DAC itself, so
 it will not stop the consensus process.
 The result shows that a numerical error due to quantization would exist
 in the distributed solution.
 It will increase as the network becomes larger.
 Thus, a numerical  technique is introduced  to mitigate the error.
 The estimated parameters after mitigation do not obviously  decline the
 performance of higher-order DAC when network size is smaller than a threshold.
 
\end_layout

\begin_layout Standard

\change_deleted 5862369 1350054120
(todo compare existing algorithm AESOPS AND PEALS ALGORITHMS in power system)
\change_unchanged

\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard

\change_deleted 936221351 1365762697
In many applications, such as time synchronization 
\begin_inset CommandInset citation
LatexCommand cite
key "Schenato2011"

\end_inset

, cooperative control of vehicles 
\begin_inset CommandInset citation
LatexCommand cite
key "Yang2010"

\end_inset

, formation control 
\begin_inset CommandInset citation
LatexCommand cite
key "Olfati-Saber2012"

\end_inset

 and WSNs 
\begin_inset CommandInset citation
LatexCommand cite
key "Hlinka2012"

\end_inset

, it is often necessary that a group of agents in a distributed system can
 agree on certain quantities.
 An example application is distributed detection of a moving target by wireless
 sensor networks.
 Suppose each sensor is observing the target coordinates but the output
 is corrupted by independent and identically distributed zero-mean Gaussian
 noise, to minimize the interference from the noise, the sensors need to
 take the average of all initial values.
 
\end_layout

\begin_layout Standard

\change_deleted 936221351 1365762697
The problem of how to achieve this average in a distributed system is called
 the 
\shape italic
average consensus problem
\shape default
,  which is solved by distributed average consensus (DAC) algorithms.
 
\end_layout

\begin_layout Standard

\change_deleted 936221351 1365762697
When the moving target is highly dynamic or the sensors need to sample at
 a very high frequency, it requires that  the DAC algorithms return the
 result in a short time.
 Thus, many efforts have been devoted to optimize the algorithms.
\change_unchanged

\end_layout

\begin_layout Standard

\change_inserted 936221351 1365764048
As introduced in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sub:To-Find-a"

\end_inset

,
\change_deleted 936221351 1365762628
 
\change_unchanged
DAC algorithms can be divided into asymptotic and non-asymptotic algorithms.

\change_deleted 936221351 1365763755
 Asymptotic algorithms have been proved to be robust against  topology changes
 and they play important roles in practice 
\begin_inset CommandInset citation
LatexCommand cite
key "Ren2007"

\end_inset

.
  The optimization of asymptotic algorithms is to minimize the sub-dominate
 eigenvalue of a weight matrix 
\begin_inset CommandInset citation
LatexCommand cite
key "Asensio-Marco2012"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Xiao2004"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Xiong2010"

\end_inset

.
  In addition, 
\begin_inset CommandInset citation
LatexCommand cite
key "Kokiopoulou2007"

\end_inset

 deals with the iteration acceleration.
 
\change_inserted 936221351 1365764011
 
\change_unchanged
However,
\change_deleted 936221351 1365763762
 
\change_inserted 936221351 1365763761
 the current
\change_unchanged
 optimization of 
\change_inserted 936221351 1365763612
asymptotic 
\change_unchanged
DAC algorithms are centralized methods
\change_inserted 936221351 1365764005
.
 The distributed method inspired by the gossip algorithm 
\begin_inset CommandInset citation
LatexCommand cite
key "Boyd2006"

\end_inset

 converges very slowly.
 
\change_deleted 936221351 1365764015
, which means a centralized node calculates the optimal parameters and forwards
 them to the whole network 
\begin_inset CommandInset citation
LatexCommand cite
key "Xiong2010"

\end_inset

.
  
\change_inserted 936221351 1365764016
 N
\change_unchanged
on-asymptotic DAC algorithms, such as finite-time 
\begin_inset CommandInset citation
LatexCommand cite
key "Sundaram2007"

\end_inset

 and adaptive filter DAC algorithm 
\begin_inset CommandInset citation
LatexCommand cite
key "Cavalcante2010"

\end_inset

, they find the average in 
\change_inserted 936221351 1365763777
finite number of iterations but not robust against topology changes
\change_unchanged
.

\change_inserted 936221351 1365764020
 
\change_deleted 936221351 1365763399
 
\change_inserted 936221351 1365763777
 
\change_deleted 936221351 1365763399
Sumdaram and Hadjicostis 
\begin_inset CommandInset citation
LatexCommand cite
key "Sundaram2007"

\end_inset

 verify that there exists a filter that can estimate the consensus value.
 Cavalcante and Mulgrew 
\begin_inset CommandInset citation
LatexCommand cite
key "Cavalcante2010"

\end_inset

 follow the work of Sundaram and Hadjicostis  to propose an adaptive algorithm
 to find the filter.
 The optimization for both of them is to minimize the number of necessary
 iterations before a FIR filter is estimated.
 However,
\change_unchanged
 
\change_deleted 936221351 1365763399
they are not robust against topology changes.
 Both of them have a first-order DAC running in the background and the local
 values over time are taken as
\change_deleted 5863457 1355325447
 
\change_deleted 936221351 1365763399
 inputs of the filter estimation algorithms.
 As a result,
\change_deleted 5863457 1355325447
 
\change_deleted 936221351 1365763399
 if the network topology changes, these algorithms have to be terminated,
 as outdated information during the filter estimation will lead to a wrong
 answer.

\change_inserted 936221351 1365763777
 
\change_unchanged

\end_layout

\begin_layout Standard

\change_deleted 936221351 1365763903
To enable the whole system to work distributively, the optimization should
 be distributed and the DAC algorithms should be robust against topology
 changes.

\change_unchanged
 
\change_deleted 936221351 1365763891
A distributed method inspired by the gossip algorithm 
\begin_inset CommandInset citation
LatexCommand cite
key "Boyd2006"

\end_inset

 can be used to optimize the first-order DAC but it converges very slowly.
   The method involves triple nested distributed matrix iterations.
 The inner iteration has to converge to a certain range so that the iteration
 outside can return the right result.
 Thus, It is not surprising that it could not finish in a reasonable time
 when the network size is large.

\change_inserted 936221351 1365763892
 
\change_unchanged

\end_layout

\begin_layout Standard

\change_deleted 5863457 1355325528
Therefore, a distributed optimization method with less computation time
 is required.
 In addition, it is better that no additional communication cost is required.
 Moreover, if the optimization algorithm and the DAC algorithms can be executed
 simultaneously, then consensus process will not be interrupted and the
 optimization can be running in background to keep the optimal parameters
 updated in a dynamic network.
 
\change_unchanged

\end_layout

\begin_layout Standard
After investigating these problems, we intend to find a distributed optimization
 method for the constant first-order DAC and higher-order DAC algorithms
 
\change_inserted 936221351 1365763942
to enable the whole system to work distributively.
 
\change_unchanged
Because in centralized optimization methods, optimal parameters of these
 algorithms are only related to the eigenvalues of Laplacian matrix of the
 network 
\begin_inset CommandInset citation
LatexCommand cite
key "Xiong2010"

\end_inset

, if we could estimate these eigenvalues in a distributed manner, these
 centralized methods could be carried out distributively.
 
\end_layout

\begin_layout Standard
Consequently, a distributed eigenvalue estimation algorithm is proposed
 in this chapter.
 In contrast to other distributed algorithms 
\begin_inset CommandInset citation
LatexCommand cite
key "Kempe2008"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Franceschelli2009"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Yang2010"

\end_inset

, initialization of the proposed algorithm is actually the first-order DAC
 itself.
 Therefore, first-order DAC will not be interrupted during the optimization
 and algorithm complexity and communication cost can be dramatically reduced.
 
\end_layout

\begin_layout Standard
However, the distributed solution has a numerical error due to quantization,
 which may decline the algorithm performance.
 Therefore, a least mean square solution is obtained to mitigate the numerical
 error.
 When using the floating point number in double format and the network size
 is smaller than 32, the numerical error after mitigation does not dramatically
 decline the performance and the proposed method is applicable.
 
\end_layout

\begin_layout Standard
The rest of this chapter is structured as follows.
 First, the distributed real-time optimization of DAC will be given in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:distributed-Eigenvalue-Estimati"

\end_inset

.
 Second, the mitigation of numerical error will be proposed in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:Mitigation-of-Numerical"

\end_inset

.
 Third, the algorithm complexity will be 
\lang british
analysed
\lang english
 in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:Algorithm-Complexity"

\end_inset

.
 Fourth, in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:Simulation-of-Eigenvalue"

\end_inset

, the performance of DAC using the distributed real-time optimization will
 be 
\lang british
analysed
\lang english
 and compared with the centralized one.
 Finally, the conclusion will be given in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:Conclusion"

\end_inset

.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:distributed-Eigenvalue-Estimati"

\end_inset

Real-time Optimization of DAC 
\end_layout

\begin_layout Standard
Traditional optimization of HO-DAC or CFO-DAC requires a centralized node
 to gather information of the Laplacian matrix 
\begin_inset CommandInset citation
LatexCommand cite
key "Xiong2010"

\end_inset

.
 Without the spectrum of Laplacian matrix, each node could only choose a
 non-optimal point 
\begin_inset Formula $\left(\epsilon,\gamma\right)$
\end_inset

 in the boundary of  the convergence region.
\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:Discrete-High-Order"

\end_inset

, it is shown that the optimal parameters 
\begin_inset Formula $\epsilon_{opt},\gamma_{opt}$
\end_inset

 of HO-DAC are only related to 
\begin_inset Formula $\lambda_{i}\left(L\right)$
\end_inset

.
 To enable the distributed optimization, the key is to estimate these eigenvalue
s in a distributed manner.
 
\end_layout

\begin_layout Standard
In fact, there are some decentralized techniques
\change_inserted 5863457 1355325722
 
\change_deleted 5863457 1355325724
 
\change_inserted 5863457 1357428859

\begin_inset CommandInset citation
LatexCommand cite
key "Yang2010"

\end_inset


\change_unchanged

\begin_inset CommandInset citation
LatexCommand cite
key "Kempe2008"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Franceschelli2009"

\end_inset

 to estimate the eigenvalues.
 However, they are not designed for DAC algorithms and will involve complex
 and costly initialization.
 In addition, 
\change_inserted 5863457 1355325845
as they are also matrix iteration algorithms, 
\change_unchanged
DAC algorithms have to be stopped during the 
\change_inserted 5863457 1355325791
eigenvalue 
\change_unchanged
estimation.
 
\end_layout

\begin_layout Standard
In contrast, the distributed real-time optimization and CFO-DAC can be running
 simultaneously
\change_deleted 936221351 1365705680
 
\change_unchanged
 and algorithm complexity and communication cost can be dramatically reduced.
 Because the initialization of the proposed algorithm is to store a number
 of local values
\change_deleted 936221351 1365705680
 
\change_unchanged
 obtained by
\change_deleted 936221351 1365705680
 
\change_unchanged
 CFO-DAC, the distributed system can still work using non-optimal parameters
 at the very beginning just after the deployment.
 After a number of iterations of CFO-DAC, these eigenvalues could be estimated
 and better parameters could be used in the next iterations.
\end_layout

\begin_layout Subsection
Find the 
\change_deleted 936221351 1365705655
Coefficients of
\change_unchanged
 Characteristic Polynomial
\end_layout

\begin_layout Standard

\change_inserted 936221351 1365705538
Because 
\begin_inset Formula $\lambda_{i}\left(W\right)$
\end_inset

 is the root of the characteristic polynomial 
\begin_inset Formula $p(\lambda)=\prod_{i=1}^{m}\left(\lambda-\lambda_{i}\right)^{r_{i}}=\lambda^{D}+a_{D-1}\lambda^{D-1}+\ldots+a_{0}=0,$
\end_inset

 the distributed eigenvalues estimation  is subject to the calculation of
 the coefficients 
\begin_inset Formula $\left\{ a_{j}\right\} $
\end_inset

.
\end_layout

\begin_layout Standard

\change_inserted 936221351 1365705490
To calculate 
\begin_inset Formula $\left\{ a_{j}\right\} $
\end_inset

, we need to use the 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:LV predictor a_i"

\end_inset

 introduced in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:Linear-Predictor-For"

\end_inset


\change_unchanged
.
 If more local values are available, node 
\begin_inset Formula $v_{i}$
\end_inset

 could list a number of equations similar to 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:x(k+r) predictor"

\end_inset

.
 Once they are sufficient to construct a matrix,
\change_inserted 936221351 1365705475
 the coefficients 
\begin_inset Formula $\left\{ a_{j}\right\} $
\end_inset

 could be estimated
\change_unchanged
.
\end_layout

\begin_layout Standard
Define the function 
\begin_inset Formula $\mathbf{y}_{i}=\mathbf{y}_{i}\left(k,D_{i}\right)=\left[x_{i}\left(k+1\right),x_{i}\left(k+2\right),\ldots,x_{i}\left(k+D_{i}\right)\right]^{T}\in\mathbb{R}^{D_{i}}$
\end_inset

 which outputs a 
\shape italic
local value history vector
\shape default
 of 
\begin_inset Formula $v_{i}$
\end_inset

 from time 
\begin_inset Formula $k+1$
\end_inset

 to 
\begin_inset Formula $k+D_{i}$
\end_inset

.
 Then, define a function 
\begin_inset Formula $T_{i}=T_{i}\left(k,D_{i}\right)\in\mathbb{R}^{D_{i}\times D_{i}}$
\end_inset

 that outputs a Toeplitz matrix with 
\begin_inset Formula $x_{i}\left(k\right)$
\end_inset

 on the diagonal.
  
\begin_inset Formula $T_{i}\left(k,D_{i}\right)=$
\end_inset


\begin_inset Formula 
\begin{align}
\left[\begin{array}{cccc}
x_{i}\left(k\right) & x_{i}\left(k-1\right) & \ldots & x_{i}\left(k-D_{i}+1\right)\\
x_{i}\left(k+1\right) & x_{i}\left(k\right) & \cdots & x_{i}\left(k-D_{i}+2\right)\\
\vdots & \vdots & \ddots & \vdots\\
x_{i}\left(k+D_{i}-1\right) & x_{i}\left(k+D_{i}-2\right) & \cdots & x_{i}\left(k\right)
\end{array}\right]\label{eq:Toeplitz_i simple}
\end{align}

\end_inset

Besides, let 
\begin_inset Formula $\mathbf{a}_{i}\left(D_{i}\right)=\left[a_{i,D_{i}-1},\ldots,a_{i,1},a_{i,0}\right]^{T}\in\mathbb{R}^{D_{i}}$
\end_inset

 be a vector  to store the coefficients calculated at 
\begin_inset Formula $v_{i}$
\end_inset

.
 Finally, we have the solution of 
\begin_inset Formula $\mathbf{a}_{i}\left(D_{i}\right)$
\end_inset

 given by
\begin_inset Formula 
\begin{equation}
\mathbf{a}_{i}\left(D_{i}\right)=-T_{i}^{-1}\left(k,D_{i}\right)\mathbf{y}_{i}\left(k,D_{i}\right).\label{eq:Toeplitz Eq.}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Toeplitz matrix is a special type of matrix and can be inverted by Levinson's
 algorithms in the polynomial time of order 
\begin_inset Formula $D_{i}^{2}$
\end_inset

, rather than the order of 
\begin_inset Formula $D_{i}^{3}$
\end_inset

 in general case (for example by LU decomposition) 
\begin_inset CommandInset citation
LatexCommand cite
key "Prass2007"

\end_inset

.
     
\end_layout

\begin_layout Subsection
Estimated Eigenvalues of Laplacian Matrix
\end_layout

\begin_layout Standard
After node 
\begin_inset Formula $v_{i}$
\end_inset

 could calculate the coefficients vector 
\begin_inset Formula $\mathbf{a}_{i}\left(D_{i}\right)$
\end_inset

, it will construct a local polynomial 
\begin_inset Formula $p_{i}\left(\lambda\right)$
\end_inset

 and find the roots of the polynomial.
 Then, the local eigenvalues spectrum of 
\begin_inset Formula $W$
\end_inset

 at 
\begin_inset Formula $v_{i}$
\end_inset

 is obtained, which
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 is defined 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
by 
\begin_inset Formula $\hat{S}_{i}\left(W\right)=\left\{ \hat{\lambda}_{j}^{\left(i\right)}\left(W\right)\right\} =\left\{ \lambda|p_{i}\left(\lambda\right)=0\right\} $
\end_inset

,  
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $j=1,\ldots,D_{i}$
\end_inset

.
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
Because 
\begin_inset Formula $W=I_{n}-\epsilon L$
\end_inset

, an estimated Laplacian spectrum at 
\begin_inset Formula $v_{i}$
\end_inset

 could be obtained, which is denoted by 
\begin_inset Formula $\hat{S}_{i}\left(L\right)=\left\{ \hat{\lambda}_{j}^{\left(i\right)}\left(L\right)\right\} $
\end_inset

, where 
\begin_inset Formula $\hat{\lambda}_{j}^{\left(i\right)}\left(L\right)=\frac{1-\hat{\lambda}_{j}^{\left(i\right)}\left(W\right)}{\epsilon},\ j=1,2.\ldots,D_{i}$
\end_inset

.
\end_layout

\begin_layout Subsection
Eigenvalues Missing In Local Spectrum 
\end_layout

\begin_layout Standard
In simulation, some eigenvalues are miss
\change_deleted 5863457 1357429069
ed
\change_inserted 5863457 1357429070
ing
\change_unchanged
 in some of the local eigenvalue
\change_deleted 936221351 1365256141
s
\change_unchanged
 spectrums.
 The reason is because sometimes the Toeplitz matrix 
\begin_inset Formula $T_{i}$
\end_inset

 with the original size 
\begin_inset Formula $D$
\end_inset

 will los
\change_deleted 5863457 1357429076
s
\change_inserted 5863457 1357429076
e
\change_unchanged
 rank,
\change_deleted 936221351 1365759138
 
\change_unchanged
 so that the solution is
\change_inserted 936221351 1365759143
 not
\change_unchanged
 unique
\change_inserted 936221351 1365759172
.
 Node 
\begin_inset Formula $v_{i}$
\end_inset

 could only build a smaller Toeplitz matrix with size 
\begin_inset Formula $D_{i}$
\end_inset

 (
\begin_inset Formula $D_{i}\leq D\leq n$
\end_inset

)
\change_unchanged
.
 As a result, the local polynomial 
\begin_inset Formula $p_{i}\left(\lambda\right)$
\end_inset

 will 
\change_inserted 5863457 1357429083
be 
\change_unchanged
reduce
\change_inserted 5863457 1357429086
d
\change_unchanged
 to 
\begin_inset Formula $D_{i}$
\end_inset

 coefficients.
 
\change_deleted 936221351 1365759193
In addition, 
\begin_inset Formula $D_{i}$
\end_inset

 may be different from one to another.

\change_inserted 936221351 1365759193
 
\change_unchanged

\end_layout

\begin_layout Standard

\change_deleted 936221351 1365759100
The example of a node 
\begin_inset Formula $v_{i}$
\end_inset

 who cannot estimate an eigenvalue 
\begin_inset Formula $\lambda_{l}$
\end_inset

 is as follows.
 Suppose 
\begin_inset Formula $\mathbf{x}\left(k\right)=W^{k}\mathbf{x}\left(0\right)=\sum_{j=1}^{n}\alpha_{j}\lambda_{j}^{k}\mathbf{e}_{j}$
\end_inset

, if for any eigenvalue 
\begin_inset Formula $\lambda_{l}=\lambda_{l}\left(W\right)$
\end_inset

, the associated eigenvector has 
\begin_inset Formula $i'th$
\end_inset

 component equaling to zero, i.e 
\begin_inset Formula $\mathbf{u}_{d}^{T}\mathbf{e}_{l}=0$
\end_inset

, where 
\begin_inset Formula $\mathbf{u}_{i}$
\end_inset

 is an all-zero vector except the 
\begin_inset Formula $i'th$
\end_inset

 component is one, then the local value at the node 
\begin_inset Formula $v_{i}$
\end_inset

, denoted by 
\begin_inset Formula $x_{i}\left(k\right)=\mathbf{u}_{i}^{T}\sum_{j=1}^{n}\alpha_{j}\lambda_{j}^{k}\mathbf{e}_{j}$
\end_inset

, will not contain any information of 
\begin_inset Formula $\lambda_{l}$
\end_inset

.
 
\change_unchanged

\end_layout

\begin_layout Standard
However, the following theorem in 
\begin_inset CommandInset citation
LatexCommand cite
key "Sundaram2007"

\end_inset

 assure that the roots of local polynomial is the same roots of minimal
 polynomial of 
\begin_inset Formula $W$
\end_inset

.
 
\end_layout

\begin_layout Theorem

\change_inserted 936221351 1365523676
\begin_inset CommandInset citation
LatexCommand cite
key "Sundaram2007"

\end_inset

 
\begin_inset CommandInset label
LatexCommand label
name "thm:local-polynomial"

\end_inset

 
\change_unchanged
The local polynomial 
\begin_inset Formula $p_{i}\left(\lambda\right)$
\end_inset

 of node 
\begin_inset Formula $v_{i}$
\end_inset

 divides the minimal polynomial 
\begin_inset Formula $p\left(\lambda\right)$
\end_inset

 of 
\begin_inset Formula $W$
\end_inset

 for all 
\begin_inset Formula $1\leq i\leq n$
\end_inset

.
 (see 
\begin_inset CommandInset citation
LatexCommand cite
key "Sundaram2007"

\end_inset


\change_inserted 936221351 1365523685
 for proof
\change_unchanged
)
\end_layout

\begin_layout Standard
Every node 
\change_inserted 936221351 1365758868
may recover the complete eigenvalue spectrum of 
\begin_inset Formula $W$
\end_inset

 
\change_deleted 936221351 1365758904
needs 
\change_inserted 936221351 1365758935
 by
\change_unchanged
 exchanging its 
\change_deleted 936221351 1365758896
estimated
\change_inserted 936221351 1365758931
 local
\change_unchanged
 eigenvalue spectrum with its neighbors
\change_deleted 936221351 1365758921
 in order to recover the full eigenvalue spectrum of 
\begin_inset Formula $W$
\end_inset


\change_unchanged
.
 To assure that no eigenvalue is missing in the final eigenvalue spectrum,
 
\change_deleted 936221351 1365758958
for 
\change_unchanged
any eigenvalue 
\begin_inset Formula $\lambda_{l}$
\end_inset


\begin_inset Formula $\left(W\right)$
\end_inset


\change_deleted 936221351 1365758960
, it
\change_unchanged
 must be estimated by at least one node in the network.
 
\end_layout

\begin_layout Theorem

\change_inserted 936221351 1365522504
\begin_inset CommandInset label
LatexCommand label
name "thm:At-least-one-node"

\end_inset

 
\change_unchanged
Suppose a graph 
\begin_inset Formula ${\cal G}$
\end_inset

 whose associated weight matrix 
\begin_inset Formula $W$
\end_inset

 satisfies the convergence condition, and initial value vector 
\begin_inset Formula $\mathbf{x}\left(0\right)\in\mathbb{R}^{n}$
\end_inset

 is chosen randomly.
 If all nodes estimate the eigenvalue of 
\begin_inset Formula $W$
\end_inset

 by the proposed method using local values that are available, any eigenvalue
 
\begin_inset Formula $\lambda_{l}\left(W\right)$
\end_inset

 could be estimated by at least one node in the network.
\end_layout

\begin_layout Proof

\change_deleted 936221351 1365759061
For the case that matrix 
\begin_inset Formula $W$
\end_inset

 is diagnosable, the proof can be easier.
 Since 
\begin_inset Formula $W^{k}=P\Lambda^{k}P^{-1}=\sum_{i=1}^{n}\lambda_{i}^{k}\mathbf{e}_{i}\mathbf{e}_{i}^{T}$
\end_inset

, we have 
\begin_inset Formula $\mathbf{x}\left(k\right)=W^{k}\mathbf{x}\left(0\right)=\sum_{i=1}^{n}\alpha_{i}\lambda_{i}^{k}\mathbf{e}_{i}$
\end_inset

, and 
\begin_inset Formula $\alpha_{i}\neq0$
\end_inset

, for all 
\begin_inset Formula $i$
\end_inset

.
 If a node 
\begin_inset Formula $v_{d}$
\end_inset

 could not estimation an eigenvalue 
\begin_inset Formula $\lambda_{l}=\lambda_{l}\left(W\right)$
\end_inset

, which means the information of 
\begin_inset Formula $\lambda_{l}$
\end_inset

 is missed in 
\begin_inset Formula $x_{d}(k)$
\end_inset

.
 This happens if and only if the 
\begin_inset Formula $d^{th}$
\end_inset

 component of 
\begin_inset Formula $\mathbf{e}_{l}$
\end_inset

 is zero, or the linear combination of eigenvectors associated with 
\begin_inset Formula $\lambda_{l}$
\end_inset

 is zero component at 
\begin_inset Formula $d$
\end_inset

.
 If no node in the network could estimate 
\begin_inset Formula $\lambda_{l}$
\end_inset

, the associated eigenvector or the linear combination of associated eigenvector
s will be 
\begin_inset Formula $\mathbf{0}_{n\times1}$
\end_inset

.
 However, this situation  will never happen  for a diagnosable matrix.
 Since diagnosable matrix has 
\begin_inset Formula $n$
\end_inset

 independent eigenvectors, any linear combination of these eigenvectors
 will not be equal to 
\begin_inset Formula $\mathbf{0}_{n\times1}$
\end_inset

.
 Therefore, there exists at least one node in the network could estimate
 
\begin_inset Formula $\lambda_{l}$
\end_inset

.
\end_layout

\begin_layout Proof
The proof can be generalized to non-diagnosable matrix with the help of
 Jordan decomposition of 
\begin_inset Formula $W$
\end_inset

.
 
\begin_inset Formula 
\begin{eqnarray*}
W^{k} & = & UJ^{k}U^{-1}\\
 & = & U\left[\begin{array}{ccc}
J_{1}^{k}\\
 & \ddots\\
 &  & J_{\hat{m}}^{k}
\end{array}\right]U^{-1}
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $\hat{m}$
\end_inset

 is the number of Jordan blocks.
 Therefore, 
\begin_inset Formula $\mathbf{x}\left(k\right)=UJ^{k}U^{-1}\mathbf{x}\left(0\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Assume that all node
\change_inserted 5863457 1357429599
s
\change_unchanged
 miss an eigenvalue 
\begin_inset Formula $\lambda_{l}$
\end_inset

 in their estimated eigenvalue spectrums, 
\begin_inset Formula $l=1,2,\ldots,\hat{m}$
\end_inset

.
 This means all the terms contain 
\begin_inset Formula $\lambda_{l}$
\end_inset

 are multiplied by zero.
 Since 
\begin_inset Formula $\mathbf{x}\left(0\right)$
\end_inset

 is chosen randomly in 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

, we can only have 
\begin_inset Formula $W^{k}=UJ^{k}U^{-1}$
\end_inset

 with all terms involving 
\begin_inset Formula $\lambda_{l}$
\end_inset

 are equal to zero, thus 
\begin_inset Formula 
\begin{equation}
U\left[\begin{array}{ccc}
\mathbf{0}\\
 & J_{l}^{k}\\
 &  & \mathbf{0}
\end{array}\right]U^{-1}=\mathbf{0}_{n\times n}\label{eq:Jordan decom.under assumption}
\end{equation}

\end_inset

Since 
\begin_inset Formula $J_{l}^{k}\neq\mathbf{0}$
\end_inset

 and the matrix 
\begin_inset Formula $U$
\end_inset

 is consist
\change_inserted 5863457 1357429608
ed
\change_unchanged
 
\change_deleted 5863457 1357429612
by
\change_inserted 5863457 1357429612
of
\change_unchanged
 linear independent vectors, the above equation 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:Jordan decom.under assumption"

\end_inset

 can not be valid.
 Thus, the assumption is not true and 
\begin_inset Formula $\lambda_{l}\left(W\right)$
\end_inset

 could be estimated by at least one node
\change_deleted 5863457 1357429629
s
\change_unchanged
 in the network.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Mitigation-of-Numerical"

\end_inset

Mitigation of Numerical Error of Eigenvalues 
\end_layout

\begin_layout Standard
Due to the limited accuracy of the floating point number, the solution obtained
 by the proposed method has a numerical error.
 To mitigate the  error, one of the options is to use floating number with
 more bits.
 However, communication cost is increased in each iteration.
 
\end_layout

\begin_layout Standard
As floating point number in double  format is  available on most computers,
 we apply our numerical mitigation  on this basis.
\end_layout

\begin_layout Standard
The idea is to have more equations similar to  
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:x(k+r) predictor"

\end_inset

 and involve the Moore-Penrose pseudo-inverse to find the least mean square
 solution.
 Thus, Toeplitz matrix in  
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:Toeplitz Eq."

\end_inset

 should be replaced by a matrix constructed by some other way, whose height
 is larger than width.
 The local value history vector 
\begin_inset Formula $\mathbf{y}_{i}\left(k,D\right)$
\end_inset

 on the left of  
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:Toeplitz Eq."

\end_inset

 is also expanded accordingly.
 
\end_layout

\begin_layout Standard
There are two ways to expand the matrix.
 First, the new matrix can be built by concatenating   Toeplitz matrix 
\begin_inset Formula $T_{i}$
\end_inset

  and 
\begin_inset Formula $T_{j},v_{j}\in{\cal N}_{i}$
\end_inset

 along the column.
  As 
\begin_inset Formula $x_{j}$
\end_inset

 is available for node 
\begin_inset Formula $v_{i}$
\end_inset

, this improvement will not increase the communication cost.
 
\end_layout

\begin_layout Standard
Second, more than one instances of CFO-DAC could be carried out to obtain
 more useful local samples.
 Let 
\begin_inset Formula $\mathbf{x}_{1}\left(0\right),\mathbf{x}_{2}\left(0\right),\ldots,\mathbf{x}_{N}\left(0\right)$
\end_inset

 denote 
\begin_inset Formula $N$
\end_inset

 different and independent initial local value vectors.
  Each one of them is used to reinitialize an instance of CFO-DAC and  
 each instance of CFO-DAC is iterated for at least 
\begin_inset Formula $2n+2$
\end_inset

 steps.
 During this initialization, each node 
\begin_inset Formula $v_{i}$
\end_inset

 will store the local values obtained by each instance of CFO-DAC.
\end_layout

\begin_layout Standard
To construct the new matrix, first let 
\begin_inset Formula $T_{i,s}=T_{i,s}\left(D_{i}-1,D_{i}\right)$
\end_inset

 be the Toeplitz matrix of 
\begin_inset Formula $v_{i}$
\end_inset

 at 
\begin_inset Formula $s$
\end_inset

 instance of CFO-DAC, with 
\begin_inset Formula $x_{i,s}\left(D_{i}-1\right)$
\end_inset

 on the diagonal, where 
\begin_inset Formula $x_{i,s}$
\end_inset

 is the local value of node 
\begin_inset Formula $v_{i}$
\end_inset

 at 
\begin_inset Formula $s$
\end_inset

 instance of CFO-DAC.
 Second, concatenating 
\begin_inset Formula $T_{i,s}$
\end_inset

 and 
\begin_inset Formula $T_{j,s},\forall v_{j}\in{\cal N}_{i}$
\end_inset

 will obtain 
\begin_inset Formula $M_{i,s}$
\end_inset

.
\begin_inset Formula 
\begin{equation}
M_{i,s}=\left[T_{i,s}^{T},T_{j_{1},s}^{T},\ldots,T_{j_{\left|{\cal N}_{i}\right|},s}^{T}\right]^{T}
\end{equation}

\end_inset

where 
\begin_inset Formula $s=1,...,N$
\end_inset

.
  Finally, concatenating the matrix 
\begin_inset Formula $M_{i,s}$
\end_inset

 will construct another matrix, 
\begin_inset Formula 
\begin{equation}
\tilde{M}_{i}=\left[M_{i,1}^{T},M_{i,2}^{T},\ldots,M_{i,N}^{T}\right]^{T}.
\end{equation}

\end_inset

The width of 
\begin_inset Formula $\tilde{M}_{i,s}$
\end_inset

, denoted by 
\begin_inset Formula $D_{i}$
\end_inset

, is the maximum integer so that the matrix 
\begin_inset Formula $\tilde{M}_{i,s}$
\end_inset

 has full column rank.
 
\end_layout

\begin_layout Standard
On the other hand, let 
\begin_inset Formula $\mathbf{y}_{i,s}=\mathbf{y}_{i,s}\left(D_{i}-1,D_{i}\right)=\left[x_{i,s}\left(D_{i}\right),x_{i,s}\left(D_{i}+1\right),\dots,x_{i,s}\left(2D_{i}\right)\right]^{T}$
\end_inset

 be the local value history vector of 
\begin_inset Formula $v_{i}$
\end_inset

 at 
\begin_inset Formula $s$
\end_inset

 instance of DAC.
 First, concatenating 
\begin_inset Formula $y_{i,s}$
\end_inset

 and 
\begin_inset Formula $y_{j,s},v_{j}\in{\cal N}_{i}$
\end_inset

 will obtain
\begin_inset Formula 
\begin{equation}
\mathbf{q}_{i,s}=\left[\mathbf{y}_{i,s}^{T},\mathbf{y}_{j_{1},s}^{T},\mathbf{y}_{j_{2},s}^{T},\ldots,\mathbf{y}_{j_{\left|{\cal N}_{i}\right|},s}^{T}\right]^{T}.
\end{equation}

\end_inset

Second, concatenating 
\begin_inset Formula $\mathbf{q}_{i,s}$
\end_inset

 will obtain
\begin_inset Formula 
\begin{equation}
\tilde{\mathbf{q}}_{i}=\left[\mathbf{q}_{i,1}^{T},\mathbf{q}_{i,2}^{T},\ldots,\mathbf{q}_{i,N}^{T}\right]^{T}.
\end{equation}

\end_inset

The vector 
\begin_inset Formula $\tilde{\mathbf{q}}_{i}$
\end_inset

 is constructed by this way so that each  local value is one time step later
 than the first  local value in each row of matrix 
\begin_inset Formula $\tilde{M}_{i}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Therefore, the coefficient vector can be obtained by inverting the new 
 matrix 
\begin_inset Formula $\tilde{M}_{i}$
\end_inset


\begin_inset Formula 
\begin{equation}
\mathbf{a}_{i}=-\tilde{M}_{i}^{+}\tilde{\mathbf{q}}_{i}\label{eq:expanded Toeplitz Eq.}
\end{equation}

\end_inset

where 
\begin_inset Formula $^{+}$
\end_inset

 denotes the Moore-Penrose pseudoinverse.
 Simulation result indicates that,  the more rows in 
\begin_inset Formula $\tilde{M}_{i}$
\end_inset

, the more accurate the solution could be.
 However, increasing the height of 
\begin_inset Formula $\tilde{M}_{i}$
\end_inset

 after a  limit 
\begin_inset Formula $n\left|{\cal N}_{i}\right|$
\end_inset

 can not obtain a more accurate result.
 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Algorithm-Complexity"

\end_inset

Analysis of Algorithm Complexity
\end_layout

\begin_layout Standard
This section is to compare the algorithm complexity of the existing and
 proposed distributed optimization  for DAC.
 
\end_layout

\begin_layout Standard
To solve the problem 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq: Opt. FO-DAC Problem"

\end_inset

, Xiao 
\begin_inset CommandInset citation
LatexCommand cite
key "Xiao2004"

\end_inset

 proposed two centralized methods:
\change_deleted 5862369 1355335771
 
\change_unchanged
 interior-point method and subgradient method to minimize 
\begin_inset Formula $\rho\left(W-\mathbf{11}^{T}/n\right)$
\end_inset

.
 Let 
\begin_inset Formula $n$
\end_inset

 be the network size and 
\begin_inset Formula $m$
\end_inset

 be the number of edges.
 The interior-point method is iterative and usually finds the optimal solution
 in 
\begin_inset Formula $20n\sim80n$
\end_inset

 step, at a cost of 
\begin_inset Formula $(10/3)n^{3}+(1/3)m^{3}$
\end_inset

 flops per steps.
 
\end_layout

\begin_layout Standard
Compared to interior-point method, the subgradient method can be computed
 locally but relatively slow.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Boyd2006"

\end_inset

 proposed a distributed subgradient method to minimize the sub-dominate
 eigenvalue of a probability matrix.
 This method can be used to optimize the 
\change_deleted 5863457 1357333397
first order DAC
\change_inserted 5863457 1357333398
first-order DAC
\change_unchanged
 after a little modification.
 The modified method is given in Algorithm 
\begin_inset CommandInset ref
LatexCommand formatted
reference "alg:Distributed-FO-DAC-Optimization"

\end_inset

, where the function 
\begin_inset Formula $\mbox{Avg}\left(u^{\left(s\right)}\right)=\frac{1}{n}\sum_{i=1}^{n}\left(u_{i}^{\left(s\right)}\right)$
\end_inset

 is implemented by DAC.
 Given the error tolerance 
\begin_inset Formula $\epsilon_{ave}$
\end_inset

, the DAC algorithm has to be iterated at least 
\begin_inset Formula $T_{ave}=O\left(n^{2}log\left(\frac{1}{\epsilon_{ave}}\right)\right)$
\end_inset

 times 
\begin_inset CommandInset citation
LatexCommand cite
key "Zhou2009"

\end_inset

, so that the local value vector can converge to the range 
\begin_inset Formula $\left\Vert \mathbf{x}\left(T_{ave}\right)-\mathbf{\bar{x}}\right\Vert \leq\epsilon_{ave}$
\end_inset

.
 The second loop to calculate sub-dominant eigenvector 
\begin_inset Formula $u_{r}$
\end_inset

 is also similar to the first iteration.
 It needs to be executed for a number of times to obtain a result within
 the error tolerance 
\begin_inset Formula $\epsilon_{sub}$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "Boyd2006"

\end_inset

.
 Furthermore, the third iteration, which is the subgradient algorithm, 
 takes enormous steps to converge and there is no simple stopping criterion
 to guarantee a certain level of optimality 
\begin_inset CommandInset citation
LatexCommand cite
key "Xiao2004"

\end_inset

.
 Therefore, with a conservative estimation, the times of matrix iteration
 might be more than 
\begin_inset Formula $O\left(n^{4}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status collapsed

\begin_layout Itemize

\series bold
Initialization: 
\series default
Initialize vector 
\begin_inset Formula $\mathbf{w}$
\end_inset

 with some feasible entries, for example the maximum degree weights.
 
\end_layout

\begin_layout Itemize

\series bold
Repeat
\series default
 for 
\begin_inset Formula $t\geq1$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $W=I-Q\mathbf{w}^{\left(t\right)}Q^{T}$
\end_inset

.
 
\end_layout

\begin_layout Itemize

\series bold
Repeat 
\series default
for
\series bold
 
\begin_inset Formula $s\geq1$
\end_inset

 
\series default
to find subgradient 
\begin_inset Formula $g^{\left(t\right)}\in\mathbb{R}^{\left|{\cal E}\right|}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $u^{\left(s+1\right)}=Wu^{\left(s\right)}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $u^{\left(s+1\right)}=u^{\left(s+1\right)}-\mbox{Avg}\left(u_{i}^{\left(s+1\right)}\right)$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $u^{\left(s+1\right)}=u^{\left(s+1\right)}/\left\Vert u^{\left(s+1\right)}\right\Vert $
\end_inset

,
\begin_inset Formula $s=s+1$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Until 
\series default
 
\series bold

\begin_inset Formula $\left\Vert u^{\left(s\right)}-u_{r}\right\Vert _{2}\leq\epsilon_{sub}$
\end_inset


\series default
.
 
\end_layout

\begin_layout Itemize
\begin_inset Formula $g_{l}=\begin{cases}
-\left(u_{i}-u_{j}\right)^{2} & \mbox{if }\rho\left(W-\frac{\mathbf{11}^{T}}{n}\right)=\lambda_{2}\left(W\right)\\
\left(u_{i}-u_{j}\right)^{2} & \mbox{if }\rho\left(W-\frac{\mathbf{11}^{T}}{n}\right)=\lambda_{n}\left(W\right)
\end{cases}$
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Formula $l\sim\left(i,j\right)\in{\cal E}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\mathbf{w}^{\left(t+1\right)}=\mathbf{w}^{\left(t\right)}-\beta_{k}\frac{g^{\left(t\right)}}{\left\Vert g^{\left(t\right)}\right\Vert }$
\end_inset

,
\begin_inset Formula $t=t+1$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:Distributed-FO-DAC-Optimization"

\end_inset

Distributively find the Optimal Matrix for FO-DAC.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Compare to the enormous number of matrix iterations in
\change_deleted 5862369 1355335787
 
\change_unchanged
 Algorithm 
\begin_inset CommandInset ref
LatexCommand formatted
reference "alg:Distributed-FO-DAC-Optimization"

\end_inset

, there is a significant time reduction by the proposed optimization if
 problem 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq: Opt. FO-DAC Problem"

\end_inset

 reduces to find the best constant.

\change_deleted 5862369 1355335787
 
\change_unchanged
 When higher accuracy is needed, we execute 
\begin_inset Formula $n$
\end_inset

 instances of CFO-DAC
\change_deleted 5862369 1355335787
 
\change_unchanged
 and the number of matrix iterations is in the order of 
\begin_inset Formula $n^{2}$
\end_inset

.
 
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
 Furthermore, if the network topology doesn't change after the optimization,
 the estimated eigenvalues  can drive to an estimated  solution 
\begin_inset Formula $\left(\hat{\epsilon},\hat{\gamma}\right)$
\end_inset

 for HO-DAC algorithms, which are faster than the optimal FO-DAC.
 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Simulation-of-Eigenvalue"

\end_inset

Simulation of Eigenvalue Estimation
\end_layout

\begin_layout Standard
The simulation 
\change_inserted 936221351 1365525005
is coded by Matlab and
\change_unchanged
 taken by the following steps.
 First, 
\begin_inset Formula $n$
\end_inset

 nodes are uniformly distributed in an unit square and a link is established
\change_deleted 936221351 1365525008
 
\change_unchanged
 between any two nodes if their distance is less than a
\change_deleted 936221351 1365525008
  
\change_unchanged
 threshold 
\begin_inset Formula $r$
\end_inset

.
 To ensure the generated graph is connected with high possibility,
\change_deleted 936221351 1365525008
 
\change_unchanged
 
\begin_inset Formula $r$
\end_inset

 is chosen as 
\begin_inset Formula $\sqrt{\log_{10}\left(n\right)/n}$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "Li2010"

\end_inset

.
 Besides, assume
\change_deleted 936221351 1365525008
 
\change_unchanged
 each link is symmetric so that the network graph is undirected.
 
\end_layout

\begin_layout Standard
Second, on the random generated network, one or more instances of CFO-DAC
  are executed.
 Local values obtained in each DAC instance are stored in local memory of
 each node.
 The step length is a constant shared by all nodes and chosen in the convergence
 range.
\end_layout

\begin_layout Standard
Third,  once sufficient number of local values are obtained, the eigenvalue
 estimation algorithm is executed and local Laplacian spectrum is obtained
 at each node.
\end_layout

\begin_layout Standard
Finally, the performance of eigenvalue estimation is evaluated by the estimation
 errors.
 Before that, each Laplacian eigenvalue 
\begin_inset Formula $\lambda_{j}\left(L\right)$
\end_inset

 is matched with only one eigenvalue 
\begin_inset Formula $\hat{\lambda}_{k}^{\left(i\right)}\left(L\right)$
\end_inset

, if the distance 
\begin_inset Formula $\left|\hat{\lambda}_{k}^{\left(i\right)}\left(L\right)-\lambda_{j}\left(L\right)\right|$
\end_inset

 is the minimum for all eigenvalues in the estimated local Laplacian spectrum.
 Let the minimum distance
\change_inserted 5863457 1357429985
 be 
\begin_inset Formula $e_{i,j}=\min_{l=1,\ldots,D_{i}}\left|\hat{\lambda}_{l}^{\left(i\right)}\left(L\right)-\lambda_{j}\left(L\right)\right|$
\end_inset


\change_unchanged
.
 The mean estimation error of 
\begin_inset Formula $\lambda_{j}\left(L\right)$
\end_inset

 is 
\begin_inset Formula $\frac{1}{n}\sum_{i=1,\ldots n}e_{i,j}$
\end_inset

.
 
\end_layout

\begin_layout Standard
In 
\change_deleted 5862369 1355761500
Fig.
\change_unchanged
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Box-plot N10 1DAC"

\end_inset

, we use the box plot to graphically illustrate the performance of eigenvalue
 estimation.
 The distribution of log mean estimation errors are obtained from 1000 simulatio
ns.
  
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide true
sideways false
status open

\begin_layout Plain Layout
\begin_inset space \hfill{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename D:/Dropbox/PaperWork/Autonomous Opt. DAC/Graph/N10_DAC1_NeiLV.pdf
	width 7cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:10-nodes,-1"

\end_inset

10 nodes, 1 DAC instance, with neighbor local values
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename D:/Dropbox/PaperWork/Autonomous Opt. DAC/Graph/N10_DAC1_NoNeiLV.pdf
	width 7cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:10-nodes,-1-1"

\end_inset

10 nodes, 1 DAC instance, no neighbor local values
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset space \hfill{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename D:/Dropbox/PaperWork/Autonomous Opt. DAC/Graph/N10_DAC2_NeiLV.pdf
	width 7cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:10-nodes,-2"

\end_inset

10 nodes, 2 DAC instance, with neighbor local values
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename D:/Dropbox/PaperWork/Autonomous Opt. DAC/Graph/N18_DAC18_NeiLV_grid.pdf
	width 7cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:18-nodes,-18"

\end_inset

18 nodes, 18 DAC instance, with neighbor local values
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Box-plot N10 1DAC"

\end_inset

Box plot of log mean estimation error of eigenvalues, with/without local
 value of neighbors.
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Simulation results show that taking local values from neighbors has better
 performance in lower estimation errors.
 
\change_inserted 5863457 1355711542
Excluding local values of neighbors will create more outliers and increase
 the estimation error.
 
\change_unchanged
In addition, the estimation errors will decrease if more instances of CFO-DAC
 are taken.
 On the other hand, the estimation errors
\change_deleted 5863457 1355711561
 
\change_unchanged
 will increase as the network size becomes larger.
 However, the numerical errors of 
\begin_inset Formula $\lambda_{2}\left(L\right)$
\end_inset

 and 
\begin_inset Formula $\lambda_{n}\left(L\right)$
\end_inset

 increase very slowly compared to other eigenvalues in the spectrum.
 Even their outliers in 
\change_deleted 5862369 1355761503
Fig.
\change_unchanged
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Box-plot N10 1DAC"

\end_inset

 have estimated error lower than 
\begin_inset Formula $10^{-8}$
\end_inset

.
\end_layout

\begin_layout Standard
To see how these estimation errors take effect on the performance of DAC,
 we conducted another simulation where estimated parameters 
\begin_inset Formula $\left(\hat{\epsilon},\hat{\gamma}\right)$
\end_inset

  are used to construct a suboptimal higher-order weight matrix 
\begin_inset Formula $\hat{H}$
\end_inset

.
 The spectral radius of 
\begin_inset Formula $\left(\hat{H}-J\right)$
\end_inset

 is plotted and compared with the optimal one in 
\change_deleted 5862369 1355761505
Fig.
\change_unchanged
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Mean-of-spectral"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset space \hfill{}
\end_inset


\begin_inset Graphics
	filename D:/Dropbox/PaperWork/Autonomous Opt. DAC/Graph/spectral_radius_Ord123_N8-32.pdf
	width 8.5cm
	BoundingBox 0bp 0bp 430bp 338bp

\end_inset


\begin_inset space \hfill{}
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Mean-of-spectral"

\end_inset

Mean of spectral radius of CFO-DAC and HO-DAC, using optimal parameters
 and estimated parameters.
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
As shown in 
\change_deleted 5862369 1355761507
Fig.
\change_unchanged
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Mean-of-spectral"

\end_inset

, CFO-DAC or SO-DAC using estimated parameters has similar spectral radius
 as the one using optimal parameters.
 It seems that the numerical errors of 
\begin_inset Formula $\lambda_{2}\left(L\right)$
\end_inset

 and 
\begin_inset Formula $\lambda_{n}\left(L\right)$
\end_inset

 do not decline their performances dramatically.
 For 
\change_deleted 5863457 1357333691
third order DAC
\change_inserted 5863457 1357333692
third-order DAC
\change_unchanged
 algorithm, estimated errors of eigenvalues don't have disastrous impact
 on the performance.
 The spectral radius goes slightly upper than the optimal one after the
 network size is larger than 25.
 It seems that estimating other eigenvalues with low accuracy is not a critical
 problem.
 However, the parameters 
\begin_inset Formula $\hat{\epsilon}$
\end_inset

 and 
\begin_inset Formula $\hat{\gamma}$
\end_inset

 might not be located in the convergence region if the network size is larger
 than 32.
 The simulation of fourth-order DAC for even larger network up to 40 nodes,
 reports several divergent cases.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Conclusion"

\end_inset


\change_deleted 936221351 1365437807
Conclusion
\change_inserted 936221351 1365437807
Summary
\change_unchanged

\end_layout

\begin_layout Standard
In this chapter, we introduced a distributed method to estimate the optimal
 parameters for DAC algorithms.
 However, numerical errors of these parameters due to quantization  can
 decline the algorithm performance.
 Especially for DAC algorithms with order larger than second, they are more
 sensitive to the errors.
 To mitigate this effect, we introduce a numerical technique to find the
 least mean square solution.
 After mitigation, the numerical errors of estimated parameters slightly
 declines the performance of 
\change_deleted 5863457 1357333401
first order DAC
\change_inserted 5863457 1357333401
first-order DAC
\change_unchanged
 and 
\change_deleted 5863457 1357333594
second order DAC
\change_inserted 5863457 1357333594
second-order DAC
\change_unchanged
.
 For the 
\change_deleted 5863457 1357333692
third order DAC
\change_inserted 5863457 1357333693
third-order DAC
\change_unchanged
, estimated parameters by local Laplacian spectrum is still convergent and
 the algorithm is faster than second order.
  However, if floating point number in double format is used and the network
 size is larger than 32 nodes, the numerical errors will be too large even
 after mitigation.
 These findings indicate that the proposed method is applicable to optimize
 higher order DAC algorithms when network size is small.
  Otherwise, we should decrease the order of DAC or increase the accuracy
 of the floating point number.
 The second-order consensus algorithm could be a compromise as it requires
 fewer parameters, converges faster than 
\change_deleted 5863457 1357333692
first order DAC
\change_inserted 5863457 1357333401
first-order DAC
\change_unchanged
 and maintains convergence reliability.
 In the future, we are intending to investigate the effect of link failure
 and other practical aspects while applying the proposed method to a distributed
 system.
 
\end_layout

\begin_layout Standard

\end_layout

\end_body
\end_document
