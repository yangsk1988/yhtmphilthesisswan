#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass IEEEtran
\options onecolumn,12pt,A4paper
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman cmr
\font_sans default
\font_typewriter default
\font_default_family rmdefault
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing onehalf
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 4cm
\topmargin 3cm
\rightmargin 2.5cm
\bottommargin 3cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Finite-Time Distributed Consensus Algorithm
\end_layout

\begin_layout Standard
Finite-Time Distributed Consensus Algorithm (FT-DCA) tries to find the consensus
 value after each node run the FO-DCA algorithm iteratively for a certain
 number of time.
 This is based on the assumption that after a certain number of time, local
 values would contain sufficient information to estimate the consensus value.
 (OK2.5)
\end_layout

\begin_layout Standard
Motivated by the works in 
\begin_inset CommandInset citation
LatexCommand cite
key "Kokiopoulou2007"

\end_inset

, we try to decomposite the local value vectors in a form which reveals
 an important property of its iteration.
 Therefore we propose a filtering technique, with which each node could
 estimate the consensus value by passing through consecutive local values
 obtained by the first order linear DAC algorithm without more information
 exchanged between nodes.
 (OK 3) (todo why we need to decomposite, because extrapolation method is
 not right.
 if not delete this line)
\end_layout

\begin_layout Subsection
Local Value Decomposition (OK 3)
\end_layout

\begin_layout Standard
Suppose an undirected network with 
\begin_inset Formula $n$
\end_inset

 nodes, where duplex communication is possible in each link.
 Therefore, the associated weight matrix 
\begin_inset Formula $\mathbf{W}\in\mathbf{R}^{n\times n}$
\end_inset

 is symmetric and diagonalizable.
 And there are 
\begin_inset Formula $n$
\end_inset

 linear independent eigenvectors of the weight matrix.

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
 Thus, any initial value vector can be written in a linear combination of
 these eigenvectors, 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\mathbf{x}\left(0\right)=\alpha_{1}\mathbf{e}_{1}+\alpha_{2}\mathbf{e}_{2}+\ldots+\alpha_{n}\mathbf{e}_{n}
\end{equation}

\end_inset

where 
\begin_inset Formula $\alpha_{i}(i=1,2,\ldots,n)$
\end_inset

 is the coefficient.
 For 
\begin_inset Formula $k=1,2,3,...$
\end_inset

 we have
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
\mathbf{x}\left(k\right) & = & \mathbf{W}^{k}\left[\alpha_{1}\mathbf{e}_{1}+\alpha_{2}\mathbf{e}_{2}+\ldots+\alpha_{n}\mathbf{e}_{n}\right]\nonumber \\
 & = & \alpha_{1}\lambda_{1}^{k}\mathbf{e}_{1}+\alpha_{2}\lambda_{2}^{k}\mathbf{e}_{2}+\ldots+\alpha_{n}\lambda_{n}^{k}\mathbf{e}_{n}\label{eq:x(k) decomposition}\\
 & = & \sum_{i=1}^{n}\alpha_{i}\lambda_{i}^{k}\mathbf{e}_{i}\nonumber 
\end{eqnarray}

\end_inset

where 
\begin_inset Formula $\lambda_{i}$
\end_inset

 is the eigenvalue of 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none

\begin_inset Formula $\mathbf{W}$
\end_inset

.
 
\end_layout

\begin_layout Standard
By rewriting 
\begin_inset Formula $\mathbf{x}\left(k\right)$
\end_inset

, it is clear that if all eigenvalues of the weight matrix are known, the
 nodes' values vector
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
 is predictable and the consensus value can be found
\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
.
 For any node 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none

\begin_inset Formula $i=1,2,\ldots n$
\end_inset

, its local value at time 
\begin_inset Formula $k$
\end_inset

 can be written as
\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
 
\begin_inset Formula 
\begin{equation}
x_{i}\left(k\right)=\alpha_{1}\lambda_{1}^{k}e_{i1}+\alpha_{2}\lambda_{2}^{k}e_{i2}+\ldots+\alpha_{n}\lambda_{n}^{k}e_{in}
\end{equation}

\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
where 
\begin_inset Formula $e_{ij}$
\end_inset

 is the 
\begin_inset Formula $i^{th}$
\end_inset

 component of eigenvector 
\begin_inset Formula $\mathbf{e}_{j}$
\end_inset

.
 Because of algebraic multiplicity of some eigenvalues, the equation can
 be modified by combining the terms with the same eigenvalues.
 Zero eigenvalues are ignored as they have no contribution in this equation.
 Suppose 
\begin_inset Formula $\mathbf{W}$
\end_inset

 has 
\begin_inset Formula $m$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
 distinct and nonzero eigenvalues, denoted by 
\begin_inset Formula $\lambda_{1},\lambda_{2},\ldots,\lambda_{m}$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
, then 
\begin_inset Formula $x_{i}\left(k\right)$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
 evolves into
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
 
\begin_inset Formula 
\begin{equation}
x_{i}\left(k\right)=\beta_{i1}\lambda_{1}^{k}+\beta_{i2}\lambda_{2}^{k}+\ldots+\beta_{im}\lambda_{m}^{k}
\end{equation}

\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
where
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
 
\begin_inset Formula $\beta_{ij}$
\end_inset

 is the coefficient after combination.
 
\end_layout

\begin_layout Subsection
Find the Consensus Value
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
Let the sample vector 
\begin_inset Formula $\mathbf{y}_{i}(k,d)\in R^{d}$
\end_inset

 is defined by the history of 
\begin_inset Formula $x_{i}(k),$
\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\mathbf{y}_{i}(k,d)=\left[x_{i}(k),x_{i}(k-1),\ldots x_{i}(k-d+1)\right]^{\mathrm{T}}\label{eq:def. y(k,m)}
\end{equation}

\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
and the Vandermonde matrix whose entries are the power of eigenvalues
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\mathbf{V}(k,d)=\left[\begin{array}{cccc}
\lambda_{1}^{k} & \lambda_{2}^{k} & \cdots & \lambda_{m}^{k}\\
\lambda_{1}^{k-1} & \lambda_{2}^{k-1} & \cdots & \lambda_{m}^{k-1}\\
\vdots & \vdots & \ddots & \vdots\\
\lambda_{1}^{k-d+1} & \lambda_{2}^{k-d+1} & \cdots & \lambda_{m}^{k-d+1}
\end{array}\right]\label{eq:def. Lamda(k,m)}
\end{equation}

\end_inset

and 
\begin_inset Formula $\mathbf{b}_{i}\left(d\right)=\left[\beta_{i1},\beta_{i2},\cdots\beta_{id}\right]^{\mathrm{T}}$
\end_inset

, then we have the following equation satisfied
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\mathbf{y}_{i}(k,d)=\mathbf{V}(k,d)\mathbf{b}_{i}\left(d\right)\label{eq:Vander matrix and consensus-d*m}
\end{equation}

\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
To obtain the consensus value 
\begin_inset Formula $\bar{x}$
\end_inset

 for node 
\begin_inset Formula $i$
\end_inset

,
\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
 we need to take sufficient samples of 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none

\begin_inset Formula $x_{i}(k)$
\end_inset


\family default
\series bold
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
and solve Eq.
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:Vander matrix and consensus-d*m"

\end_inset

, where 
\begin_inset Formula $d$
\end_inset

 should at least equal or larger than
\family default
\series bold
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
 
\begin_inset Formula $m$
\end_inset


\series default
.
 
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
Let 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit

\begin_inset Formula $A(k,m)=\mathbf{V}^{-1}(k,m)$
\end_inset

 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
and
\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
 the first row of 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none

\begin_inset Formula $A(k,m)$
\end_inset

 is given by
\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none

\begin_inset Formula 
\begin{equation}
\mathbf{h}=\left[A_{11}(k,m),A_{12}(k,m),\ldots,A_{1m}(k,m)\right]^{\mathrm{T}}
\end{equation}

\end_inset

 we have 
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
\begin_inset Formula 
\begin{equation}
\beta_{i1}=\mathbf{h}^{\mathrm{T}}\mathbf{y}_{i}(k,m)\label{eq:Find Consensus m order}
\end{equation}

\end_inset

 Eq.
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:Find Consensus m order"

\end_inset

 shows that the consensus value can be calculated 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
by the filter defined in Def.
\begin_inset CommandInset ref
LatexCommand ref
reference "Def.A-consensus-finding"

\end_inset

.
 
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
It is worth noting that Vandermonde matrix is related to a polynomial interpolat
ion problem and can be easily inverted in terms of Lagrange basis polynomials
 
\begin_inset CommandInset citation
LatexCommand cite
key "Prass2007"

\end_inset

.

\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
 Due to this reason, this method can be treated as an extrapolation method
 which find the consensus value at infinity.
 At the same time, we only need to find out the first coefficient 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\beta_{i1}$
\end_inset

 in the distributed averaging.
 Therefore, 
\strikeout default
\uuline default
\uwave default
only the elements in the corresponding row of 
\strikeout off
\uuline off
\uwave off

\begin_inset Formula $\mathbf{V}^{-1}(k,m)$
\end_inset


\strikeout default
\uuline default
\uwave default
 need to be found.
 This approach can save lots of computation time when using the algorithm
 in 
\begin_inset CommandInset citation
LatexCommand cite
key "Prass2007"

\end_inset

.
 
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
Since 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
simulation result shows that 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none

\begin_inset Formula $\mathbf{h}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
 is only depends the associated Vandermonde matrix
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
 
\begin_inset Formula $\mathbf{V}(k,m)$
\end_inset

 which is 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
is independent of the nodes' initial values and time index 
\begin_inset Formula $k$
\end_inset

.
 Therefore, 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
each node in the network could find the consensus value at any time 
\begin_inset Formula $k\geqslant m$
\end_inset

 by passing a number of consecutive local values through this filter.

\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
 In addition, all nodes in this network may share the same filter, which
 means all the filters have the same impulse response.
 However, such a consensus finding filter it is not unique.
 As an example, a number of filters which have different impulse response
 or filter lengths are found by different samples 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
of 
\begin_inset Formula $x_{i}(k)$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
.
  Each node could choose its own, but filter length determines the how many
 time-steps before a node could find 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
the consensus value.
 
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
Suppose we take 
\begin_inset Formula $d$
\end_inset

 samples of 
\begin_inset Formula $x_{i}(k)$
\end_inset

, where 
\begin_inset Formula $d>m$
\end_inset

.
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
Because
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
 the Vandermonde matrix 
\begin_inset Formula $\mathbf{V}(k,d)\in R^{d\times m}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
 is non-square, we introduce the Moore-penrose pseudo inverse to find the
 least mean square solution.
 
\end_layout

\begin_layout Standard
Let
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
A(k,d)=\mathbf{V}^{+}(k,d)
\end{equation}

\end_inset

where 
\begin_inset Formula $^{+}$
\end_inset

 denote the Moore-penrose pseudo inverse.
 And
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
 the first row of 
\begin_inset Formula $A(k,d)$
\end_inset

 is given by 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\mathbf{h}'=\left[A{}_{11}(k,d),A_{12}(k,d),\ldots,A_{1d}(k,d)\right]^{\mathrm{T}}
\end{equation}

\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
Still, the value 
\strikeout off
\uuline off
\uwave off

\begin_inset Formula $\beta_{i1}=\mathbf{h}'^{\mathrm{T}}\mathbf{y}_{i}(k,m)$
\end_inset

 
\strikeout default
\uuline default
\uwave default
is an accurate estimation of consensus value.
 Therefore, another consensus finding filter is obtained.
\end_layout

\begin_layout Standard
Due to the multiplication of the consensus finding filter, the set of filter
 is defined by
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
H=\{\mathbf{h}\in R^{d}|\forall d\geqslant m,\:\mathbf{h}^{\mathrm{T}}\mathbf{y}_{i}(k,d)=\bar{x}\}
\end{equation}

\end_inset

However, the shortest filter has its length equal to 
\begin_inset Formula $m$
\end_inset

, which means node can only have the consensus value after 
\begin_inset Formula $m$
\end_inset

 steps.
 
\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sub:Numerical-Simulation"

\end_inset

, the performance of this algorithm is shown by comparing it with the first
 order DAC algorithm using optimal weight matrix in 
\begin_inset CommandInset citation
LatexCommand cite
key "Xiao2004"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Solve the Vandermonder matrix 
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
(todo)Solving Vandermonde matrix is related to a polynomial interpolation
 problem and can be easily inverted in terms of Lagrange basis polynomials
 
\end_layout

\begin_layout Subsubsection
\begin_inset CommandInset label
LatexCommand label
name "sub:Numerical-Simulation"

\end_inset

Numerical Simulation
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset space \hfill{}
\end_inset


\begin_inset Graphics
	filename ../Distributed Consensus Algorithm/Report_DAC/Net_Weight/graph with 8 nodes and 17 edges.pdf

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Graph in Xiao'paper"

\end_inset

Graph with optimal weights which maximize convergence rate 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Consider the graph from 
\begin_inset CommandInset citation
LatexCommand cite
key "Xiao2004"

\end_inset

, the weight matrix 
\series bold

\begin_inset Formula $\mathbf{W}$
\end_inset


\series default
 corresponding to this graph is symmetric and has eigenvalues 
\begin_inset Formula $\lambda(\mathbf{W})=\{1,0.6,0.4,0,0,0,-0.4,-0.6\}$
\end_inset

.
 The time index 
\begin_inset Formula $k$
\end_inset

 can be chosen large enough so that there are only positive powers in the
 matrix 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none

\begin_inset Formula $\mathbf{V}(k,d)$
\end_inset

.
 For example, there are
\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
 5 distinct and nonzero eigenvalues of 
\series bold

\begin_inset Formula $\mathbf{W}$
\end_inset


\series default
, so we choose 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
the time index 
\begin_inset Formula $k=5$
\end_inset

 and 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit

\begin_inset Formula $d=5$
\end_inset

 which is the minimum filter length
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
.
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
\begin_inset Formula 
\[
\mathbf{V}(5,5)=\left[\begin{array}{ccccc}
1 & 0.0778 & 0.0102 & -0.0102 & -0.0778\\
1 & 0.1296 & 0.0256 & 0.0256 & 0.1296\\
1 & 0.216 & 0.064 & -0.064 & -0.216\\
1 & 0.36 & 0.16 & 0.16 & 0.36\\
1 & 0.6 & 0.4 & -0.4 & -0.6
\end{array}\right]
\]

\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
 the first row of the inverse matrix 
\begin_inset Formula $\mathbf{V}^{-1}(5,5)$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
gives the consensus finding filter 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbf{h}=\left[1.8601,\;0,\;-0.9673,\;0,\;0.1071\right]^{\mathrm{T}}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset space \hfill{}
\end_inset


\begin_inset Graphics
	filename ../Distributed Consensus Algorithm/Report_DAC/MSE/MSE_Filter vs FODAC.eps
	width 8cm

\end_inset


\begin_inset space \hfill{}
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "cap:perform. Consensus Filter"

\end_inset

Performance of the first order iteration with optimal matrix vs.
 consensus finding filter algorithm
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
For any random generated 
\begin_inset Formula $\mathbf{x}(0)\in R^{n}$
\end_inset

, node values vector 
\begin_inset Formula $\mathbf{x}(k)$
\end_inset

 is updated by the iteration Eq.
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:first order matrix"

\end_inset

.
 At the same time each node passes its local values though filter 
\begin_inset Formula $\mathbf{h}$
\end_inset

.
 Filter output is given by 
\begin_inset Formula $\hat{x}_{i}(k)=\mathbf{h}(k)*x_{i}(k)$
\end_inset

.
 Fig.
\begin_inset CommandInset ref
LatexCommand ref
reference "cap:perform. Consensus Filter"

\end_inset

 compares the first order DAC (FO-DAC) algorithm with optimal matrix and
 the proposed algorithm with consensus finding filter.
 The performance is evaluated by the mean square error (MSE), defined by
 
\begin_inset Formula $\mbox{MSE}_{\mbox{FO-DAC}}(k)=\sum_{i\in\mathcal{N}}E[\left|x_{i}(k)-\bar{x}\right|^{2}]$
\end_inset

, 
\begin_inset Formula $\mbox{MSE}_{\mbox{filter}}(k)=\sum_{i\in\mathcal{N}}E[\left|\alpha_{i}(k)-\bar{x}\right|^{2}]$
\end_inset

 respectively, where 
\begin_inset Formula $\bar{x}=(1/n)\sum_{i\in\mathcal{N}}x_{i}(0)$
\end_inset

.
 The result shows that the consensus finding filter calculate the consensus
 value after a finite number of iteration and MSE drops dramatically to
 the quantization error at the same time.
 
\end_layout

\begin_layout Section
Consensus Based Signal Processing
\end_layout

\begin_layout Standard
In the last section(todo), we introduced the FO-DCA and FT-DCA algorithm.
 Based on these algorithm, actually each node can extract more information
 from local values sequence, if some signal processing techniques are used.
 These signal processing techniques introduced in the following may also
 be applied to wireless sensor networks as they could be carried out distributiv
ely.
 
\end_layout

\begin_layout Subsection
Consensus Based Network Information Flooding(todo)
\end_layout

\begin_layout Standard
The conventional information flooding actually is done by copy information
 to all other nodes in the network.
 Each node maintains a table of the values of all nodes in the network,
 initialized with its own node value.
 And nodes exchanges the tables of their own and those from their neighbors
 in each iteration.
 After a certain number of iterations which is equal to the diameter of
 the network, each node will knows all the nodes's value.
 It is also a method to implement distributed averaging.
 
\end_layout

\begin_layout Standard
However, copying information and forwarding to all other ondes takes too
 much resources for the networks.
 If a networks has 
\begin_inset Formula $n$
\end_inset

 nodes, the conventional flooding will needs at least 
\begin_inset Formula $\left(n-1\right)$
\end_inset

 copies for each piece of information.
 And this estimation doesn't considering the cost in transmitting the informatio
n to the destination.
 Therefore, in this section we propose a novel network information flooding
 technique based on consensus algorithm.
 It doesn't require copying information for many time, but transimit information
 by broadcasting.
 The advantage of this method is that it can save the costs in copying and
 trainsmitting information.
 
\end_layout

\begin_layout Standard
Suppose the network weight matrix is 
\begin_inset Formula $\mathbf{W}$
\end_inset

 that satisfies the same condition in section (todo).
 Recall the intial local value decomposition in section (todo)
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula 
\begin{equation}
\mathbf{x}\left(0\right)=\sum_{j=1}^{n}\alpha_{j}\mathbf{e}_{j}\label{eq:initial vector decompose}
\end{equation}

\end_inset

which shows that the 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
intial
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 local value vector can be defined by a set of coefficients and a new basis
 defined by the eigenvectors.

\family default
\series bold
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 
\series default
In addition, the eigenvectors 
\begin_inset Formula $\mathbf{e}_{i}$
\end_inset

 are only depends on the weight matrix.
 Therefore,
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 once the coefficients 
\begin_inset Formula $\alpha_{i}$
\end_inset

 and weight matrix are available, the initial values vector 
\begin_inset Formula $\mathbf{x}\left(0\right)$
\end_inset

 can be caculated.
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
It is possible to exchange information between nodes in the network without
 copying information to all other nodes.
\end_layout

\begin_layout Standard
To show how this method can be carried out distributively, recall the FT-DCA
 algorithm in section (todo)
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
.
 Let the sample vector 
\begin_inset Formula $\mathbf{y}_{i}(k,d)\in R^{d}$
\end_inset

 is defined by the history of 
\begin_inset Formula $x_{i}(k),$
\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\mathbf{y}_{i}(k,d)=\left[x_{i}(k),x_{i}(k-1),\ldots x_{i}(k-d+1)\right]^{\mathrm{T}}
\end{equation}

\end_inset

and the coefficients vector 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\mathbf{a}=\left[\alpha_{1},\alpha_{2},\ldots,\alpha_{n}\right]^{T}$
\end_inset

.

\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 If we involve the eigenvectors and redefine the 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:Vander matrix and consensus-d*m"

\end_inset

 by 
\begin_inset Formula 
\begin{equation}
\mathbf{y}_{i}(k,d)=\mathbf{V}_{i}(k,d)diag\left(\left[\mathbf{e}_{1}^{T}\mathbf{u}_{i},\mathbf{e}_{2}^{T}\mathbf{u}_{i},\ldots,\mathbf{e}_{n}^{T}\mathbf{u}_{i}\right]\right)\mathbf{a}\label{eq:Vander*Eigen i^th*Alpha}
\end{equation}

\end_inset

where
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 
\begin_inset Formula $\mathbf{u}_{i}$
\end_inset

 is the unit vector with all zero except the 
\begin_inset Formula $i^{th}$
\end_inset

 component is one, 
\begin_inset Formula $\mathbf{e}_{j}^{T}\mathbf{u}_{i}$
\end_inset

 means the 
\begin_inset Formula $i^{th}$
\end_inset

 component of 
\begin_inset Formula $\mathbf{e}_{j}$
\end_inset

.
 Solving the above equation will obtain the coefficients 
\begin_inset Formula $\alpha_{j}$
\end_inset

.
 
\end_layout

\begin_layout Standard
The consensus based information flooding is idealy suitable for time-invariant
 network.
 Because a node must know all the eigenvectors and eigenvalues of the network
 before it can estimate other nodes' initial value.
 This requires each node flooding its weight coefficients to all nodes in
 the network at first.
 However, instead of flooding the table of initial values, flooding the
 weight matrix is only performed at the stage of network initializaion or
 when the network topology changes.
 If the frequency of topology is very low or in a range that acceptable,
 the proposed method could have lower cost in computation and communication.
\end_layout

\begin_layout Subsection
Linear Predictor For Local Value (todo check 0)
\end_layout

\begin_layout Standard
In observing the convergence behaviour of each node's local value sequance,
 one would come to the idea that the sequence must obays some rule when
 it convergence.
 In section (todo), we already shows that local value vector can be decomposed
 in terms of eigenvalues and eigenvectors.
 Based on this fact, a consensus estimation method by invering the Vandermonde
 matrix and a information flooding techniques are proposed.
\end_layout

\begin_layout Standard
However, another important property of the FO-DCA need to be highlighted
 in this section, because it will also inspire some ideas on signal processing
 and applications in distributed systems.
\end_layout

\begin_layout Standard
To see how this property can be explained in a equation, we use the concept
 of minimal polynomial of the weight matrix 
\begin_inset Formula $\mathbf{W}$
\end_inset

.
 For any weight matrix 
\begin_inset Formula $\mathbf{W}$
\end_inset

, it has distinct eigenvalues denoted by 
\begin_inset Formula $\lambda_{1},\lambda_{2},\ldots,\lambda_{m},$
\end_inset

 then the minimal polynomial can be obtained by 
\begin_inset Formula 
\[
p(\lambda)=\prod_{i=1}^{m}\left(\lambda-\lambda_{i}\right)^{r_{i}}
\]

\end_inset

where 
\begin_inset Formula $r_{i}$
\end_inset

 is the size of the largest Jordan block of 
\begin_inset Formula $\mathbf{W}$
\end_inset

 corresponding to eigenvalue 
\begin_inset Formula $\lambda_{i}$
\end_inset

.
 The minimal polynomial can also be expanded into the form 
\begin_inset Formula 
\[
p\left(\lambda\right)=\lambda^{r+1}+a_{r}\lambda^{r}+\ldots+a_{1}\lambda+a_{0}
\]

\end_inset

where 
\begin_inset Formula $r+1$
\end_inset

 is the degree of the minimal polynomial.
 Since the minimal polynomial of matrix 
\begin_inset Formula $\mathbf{W}$
\end_inset

 satisfies 
\begin_inset Formula $p\left(\mathbf{W}\right)=0$
\end_inset

.
 We have
\begin_inset Formula 
\[
\mathbf{W}^{r+1}+a_{r}\mathbf{W}^{r}+\ldots+a_{1}\mathbf{W}+a_{0}\mathbf{I}=0
\]

\end_inset

if we multiply each side with intial value vector, and use the fact 
\begin_inset Formula $\mathbf{x}\left(k+1\right)=\mathbf{W}^{k+1}\mathbf{x}\left(0\right)$
\end_inset

, the above equation evolves into
\begin_inset Formula 
\[
\mathbf{x}\left(r+1\right)+a_{r}\mathbf{x}\left(r\right)+\ldots+a_{1}\mathbf{x}\left(1\right)+a_{0}\mathbf{x}\left(0\right)=0
\]

\end_inset

Therefore, for any 
\begin_inset Formula $k\geq r$
\end_inset


\begin_inset Formula 
\begin{equation}
\mathbf{x}\left(k+1\right)=-a_{r}\mathbf{x}\left(k\right)-\ldots-a_{1}\mathbf{x}\left(k-r+1\right)+a_{0}\mathbf{x}\left(k-r\right)\label{eq:local value linear predictor}
\end{equation}

\end_inset

This equation shows that the vector sequence can be predict by a FIR filter
 with coefficient given by 
\begin_inset Formula $\left[-a_{r},-a_{r-1},\ldots,-a_{0}\right].$
\end_inset


\end_layout

\begin_layout Standard
Given the local value vector sequence obtained by FO-DCA, one may instantly
 comes to the idea of applying a adaptive filter algorithm to estimate the
 set of coefficient.
 For example, LMS, LSL and Kalman filter.
 One advantage of the adaptive filter algorithm is that when the network
 topology is changed, the filter could adaptively change its coefficient
 during the iteration.
 
\end_layout

\begin_layout Standard
In the next, we will show that the relationship of Linear Predictor for
 Local Value and Linear Predictor for Consensus value.
\end_layout

\begin_layout Subsubsection
Relationship With The Consensus Finding Filter
\end_layout

\begin_layout Standard
(todo show the linear predictor equation ain nd obtain the consensus estimation
 equation.) 
\end_layout

\begin_layout Standard
As shown in section (todo), the consensus finding filter is given by the
 first row of inverse of Vandermonde's matrix.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $V_{n}$
\end_inset

 be the vandermonde's matrix of order 
\begin_inset Formula $n$
\end_inset

 given by, 
\begin_inset Formula $n$
\end_inset

 is the distinct eigenvalue of weight matrix 
\begin_inset Formula $\mathbf{W}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
V_{r}=\left[\begin{array}{cccc}
\lambda_{1} & \lambda_{2} & \cdots & \lambda_{n}\\
\lambda_{1}^{2} & \lambda_{2}^{2} & \cdots & \lambda_{n}^{2}\\
\vdots & \vdots & \ddots & \vdots\\
\lambda_{1}^{n} & \lambda_{2}^{n} & \cdots & \lambda_{n}^{n}
\end{array}\right]\label{eq:Vander Matrix}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
then its inverse is closely related to Lagrange's polynomial interpolation
 formula.
 
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula 
\[
P_{j}\left(x\right)=\prod_{\begin{array}{c}
n=1\\
n\neq j
\end{array}}^{N}\frac{x-\lambda_{n}}{\lambda_{j}-\lambda_{n}}=\sum_{k=1}^{N}b_{jk}x^{k-1}
\]

\end_inset


\end_layout

\begin_layout Standard
the polynomial 
\begin_inset Formula $P_{j}\left(x\right)$
\end_inset

 is sepcially designed so that it takes on a value of zero at all 
\begin_inset Formula $\lambda_{i}$
\end_inset

 with 
\begin_inset Formula $i\neq j$
\end_inset

 and has a value of unity at 
\begin_inset Formula $x=\lambda_{j}$
\end_inset

.
 In other words, 
\begin_inset Formula 
\[
P_{j}\left(x_{i}\right)=\delta_{ij}=\sum_{k=1}^{N}b_{jk}x^{k-1}
\]

\end_inset


\end_layout

\begin_layout Standard
the equation says that 
\begin_inset Formula $b_{ik}$
\end_inset

 is exactly the inverse of the matrix 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:Vander Matrix"

\end_inset

, with the subscript 
\begin_inset Formula $k$
\end_inset

 as the column index.
 Then 
\begin_inset Formula $b_{ik}$
\end_inset

 can be specified as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
b_{ik}=\left(-1\right)^{k-1}\left(\frac{\sum_{\begin{array}{c}
1\leqslant m_{1}<\ldots<m_{n-k}\leqslant n\\
m_{1},\ldots,m_{n-k}\neq i
\end{array}}\lambda_{m_{1}}\cdots\lambda_{m_{n-j}}}{\prod_{\begin{array}{c}
1\leqslant v\leqslant n\\
v\neq i
\end{array}}\left(\lambda_{v}-\lambda_{i}\right)}\right)
\]

\end_inset

Thus, the first row of the inverse of Vandermonde's Matrix, which is also
 the consensus finding filter
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbf{h}=\left[b_{11},b_{12},\ldots,b_{1n}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
Recall the minimal polynomial of weight matrix.
 Since weight matrix has only 
\begin_inset Formula $m$
\end_inset

 distinct eigenvalues, its minimal polynomial can be caculated by 
\begin_inset Formula 
\[
p(\lambda)=\prod_{i=1}^{m}\left(\lambda-\lambda_{i}\right)=\lambda^{r+1}+a_{r}\lambda^{r}+\ldots+a_{1}\lambda+a_{0}
\]

\end_inset


\end_layout

\begin_layout Standard
If matrix 
\begin_inset Formula $\mathbf{W}$
\end_inset

 satisfies the condition in Theorem (todo), there is a simple eigenvalue
 equals to one.
 Without loss of generality, let the first eigenvalue 
\begin_inset Formula $\lambda_{1}=1$
\end_inset

, and define another polynomial 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $q\left(\lambda\right)$
\end_inset

 by
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 
\begin_inset Formula 
\[
q\left(\lambda\right)=\frac{p\left(\lambda\right)}{\left(\lambda-1\right)}=\prod_{i=2}^{m}\left(\lambda-\lambda_{i}\right)=
\]

\end_inset

and 
\begin_inset Formula $q\left(1\right)=$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\prod_{i=2}^{m}\left(1-\lambda_{i}\right)$
\end_inset


\end_layout

\begin_layout Subsection
Consensus Based Distributed Eigenvalues Estimation(todo)
\end_layout

\begin_layout Standard
eigenvalues of the network are vital to optimize the convergence rate of
 FO-DCA and HO-DCA.
 We proposed an algorithm to estimate these eigenvalues based on FO-DCA.
\end_layout

\end_body
\end_document
