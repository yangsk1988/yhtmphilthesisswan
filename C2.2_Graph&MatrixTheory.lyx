#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass book
\options 12pt,A4paper,onecolumn
\use_default_options true
\begin_modules
eqs-within-sections
figs-within-sections
tabs-within-sections
theorems-ams-bytype
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman cmr
\font_sans default
\font_typewriter default
\font_default_family rmdefault
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing onehalf
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 4cm
\topmargin 3cm
\rightmargin 2.5cm
\bottommargin 3cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\author 5862369 "HT" 
\end_header

\begin_body

\begin_layout Section
Graph Theory and Matrix Theory Review 
\end_layout

\begin_layout Standard
In this section, some basic concepts of the graph theory and matrix theory
 will be introduced.
 They are used in the analysis of convergence or performance of consensus
 algorithm.
 Because consensus algorithm actually relates to a matrix iteration algorithm,
 it is necessary to introduce some of these theorems.
 For full information about matrix theory, see 
\begin_inset CommandInset citation
LatexCommand cite
key "Varga2010"

\end_inset

, and the work 
\begin_inset CommandInset citation
LatexCommand cite
key "Russell1994"

\end_inset

 states more details about Laplacian matrix.
 However, some useful properties of Laplacian matrix will to be introduced
 here.
 
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula ${\cal G}=\left({\cal V},{\cal E},{\cal A}\right)$
\end_inset

 be a graph with 
\begin_inset Formula $n$
\end_inset

 nodes.
 The in-degree and out-degree of node 
\begin_inset Formula $i$
\end_inset

 are defined by:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
D_{in}\left(i\right)=\sum_{i=1}^{n}a_{i,j}
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
D_{out}\left(i\right)=\sum_{j=1}^{n}a_{i,j}
\end{equation}

\end_inset

where 
\begin_inset Formula $a_{i,j}$
\end_inset

 is the elements of matrix 
\begin_inset Formula ${\cal A}$
\end_inset

.
 This definition states that in-degree of node 
\begin_inset Formula $i$
\end_inset

 is the 
\begin_inset Formula $i^{th}$
\end_inset

 column sum of matrix 
\begin_inset Formula ${\cal A}$
\end_inset

 and the out-degree of node 
\begin_inset Formula $i$
\end_inset

 is the 
\begin_inset Formula $i^{th}$
\end_inset

 row sum of matrix 
\begin_inset Formula ${\cal A}.$
\end_inset

 And the graph Laplacian matrix 
\begin_inset Formula ${\cal L}$
\end_inset

 induced by the 
\begin_inset Formula ${\cal G}$
\end_inset

 is the same as defined before, see Eq.
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:Graph Laplacian def."

\end_inset

.
 In addition, we can find the relationship of 
\begin_inset Formula ${\cal L}$
\end_inset

 and 
\begin_inset Formula ${\cal A}$
\end_inset

.
 
\begin_inset Formula 
\begin{equation}
{\cal L}=\Delta-{\cal A}
\end{equation}

\end_inset

 where 
\begin_inset Formula $\Delta$
\end_inset

 is a diagonal matrix 
\begin_inset Formula $\Delta=\left[\Delta_{i,j}\right]$
\end_inset

, 
\begin_inset Formula $\Delta_{i,j}=0$
\end_inset

 for all 
\begin_inset Formula $i\neq j$
\end_inset

 and 
\begin_inset Formula $\Delta_{ii}=D_{out}\left(i\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Note that we assume the diagonal elements 
\begin_inset Formula $a_{i,i}$
\end_inset

 of matrix 
\begin_inset Formula ${\cal A}$
\end_inset

 equal to zero for all 
\begin_inset Formula $i$
\end_inset

.
 Thus, the Laplacian matrix is only dependent on the off-diagonal elements
 of 
\begin_inset Formula ${\cal A}$
\end_inset

.
 Moreover, if we assume matrix 
\begin_inset Formula ${\cal A}$
\end_inset

 is non-negative, we can benefit from the properties of non-negative matrix,
 and use them in optimization of convergence rate of consensus algorithm.
\end_layout

\begin_layout Subsection
Irreducibility and Strong Connected Graph.
\end_layout

\begin_layout Standard
For an undirected graph, the consensus 
\begin_inset Formula $\mathbf{x}^{*}$
\end_inset

 can be achieved if and only if the graph is connected.
 (Note that the consensus is a stable state of the system dynamic.
 For the average consensus problem the consensus state is a state where
 all the network node converge to the global average.) But for the directed
 graph, this condition of achieving consensus becomes into if and only if
 the graph is strongly connected.
 
\end_layout

\begin_layout Standard
A directed graph is strongly connected if and only if for any two distinct
 nodes 
\begin_inset Formula $i$
\end_inset

 and 
\begin_inset Formula $j$
\end_inset

, there exists a path that follows the direction of the edges and connects
 
\begin_inset Formula $i$
\end_inset

 and 
\begin_inset Formula $j$
\end_inset

 on the digraph.
 
\end_layout

\begin_layout Definition
\begin_inset CommandInset label
LatexCommand label
name "Def.Irreducebility"

\end_inset

for 
\begin_inset Formula $n>1$
\end_inset

, an 
\begin_inset Formula $n\times n$
\end_inset

 matrix 
\begin_inset Formula $A\in R^{n\times n}$
\end_inset

 is reducible if there exists an 
\begin_inset Formula $n\times n$
\end_inset

 permutation matrix 
\begin_inset Formula $P$
\end_inset

 such that 
\begin_inset Formula $PAP^{T}$
\end_inset

 is in block upper triangular form.
 
\begin_inset Formula 
\begin{equation}
PAP^{T}=\left[\begin{array}{cc}
A_{1,1} & A_{1,2,}\\
O & A_{2,2}
\end{array}\right]
\end{equation}

\end_inset

where 
\begin_inset Formula $A_{1,1}$
\end_inset

 is an 
\begin_inset Formula $r\times r$
\end_inset

 submatrix and 
\begin_inset Formula $A_{2,2}$
\end_inset

 is an 
\begin_inset Formula $\left(n-r\right)\times\left(n-r\right)$
\end_inset

 submatrix, and 
\begin_inset Formula $O$
\end_inset

 is an null matrix, 
\begin_inset Formula $1\leq r<n$
\end_inset

.
 If no such a permutation matrix exists, the matrix 
\begin_inset Formula $A$
\end_inset

 is irreducible.
 If 
\begin_inset Formula $n=1$
\end_inset

, then 
\begin_inset Formula $A$
\end_inset

 is reducible if 
\begin_inset Formula $A=0$
\end_inset

, and irreducible otherwise.
 
\end_layout

\begin_layout Standard
The relationship of the irreducible property of matrix 
\begin_inset Formula $A$
\end_inset

 and the strong connected property of directed graph 
\begin_inset Formula ${\cal G}\left(A\right)$
\end_inset

 is stated by the following theorem.
\end_layout

\begin_layout Theorem
An 
\begin_inset Formula $n\times n$
\end_inset

 complex matrix 
\begin_inset Formula $A\in C^{n\times n}$
\end_inset

 is irreducible if and only if its directed graph 
\begin_inset Formula ${\cal G}\left(A\right)$
\end_inset

 is strongly connected.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Varga2010"

\end_inset


\end_layout

\begin_layout Standard
The proof of this theorem is obvious.
 If a graph is strongly connected, all the off diagonal elements of graph
 matrix 
\begin_inset Formula $A$
\end_inset

 cannot be vanished by matrix permutation.
 Therefore, matrix 
\begin_inset Formula $A$
\end_inset

 doesn't exists the block upper triangular form as given in Def.
 
\begin_inset CommandInset ref
LatexCommand formatted
reference "Def.Irreducebility"

\end_inset

.
 
\end_layout

\begin_layout Subsection
Spectral radius of a matrix
\end_layout

\begin_layout Standard
Spectral radius of a matrix is one of the basic concepts in the matrix iteration
 theory.
 It is defined by the largest eigenvalue of the matrix.
 The matrix iteration is very useful in many applications, denoted by 
\begin_inset Formula $A,A^{2},A^{3},\ldots$
\end_inset

.
 The power sequence is said to be convergent, if and only if 
\begin_inset Formula $\lim_{k\to\infty}A^{k}=O$
\end_inset

, where 
\begin_inset Formula $O$
\end_inset

 is a zero matrix with all zero entries.
 The following theorem states that the convergent property is strongly connected
 with the spectral radius.
 
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:Convergent <=> p(A)<1"

\end_inset

 if 
\begin_inset Formula $A\in C^{n\times n}$
\end_inset

 is an 
\begin_inset Formula $n\times n$
\end_inset

 complex matrix, then 
\begin_inset Formula $A$
\end_inset

 is convergent if and only if 
\begin_inset Formula $\rho\left(A\right)<1.$
\end_inset


\end_layout

\begin_layout Proof
The proof uses the Jordan form of a matrix.
 For any matrix 
\begin_inset Formula $A\in C^{n\times n}$
\end_inset

, there exists a nonsingular 
\begin_inset Formula $n\times n$
\end_inset

 matrix 
\begin_inset Formula $T$
\end_inset

, such that 
\begin_inset Formula $A$
\end_inset

 reduces into the Jordan normal form 
\begin_inset Formula 
\begin{equation}
T^{-1}AT=J=\left[\begin{array}{cccc}
J_{1} &  &  & O\\
 & J_{2}\\
O &  & \ddots & J_{m}
\end{array}\right]
\end{equation}

\end_inset

 where each Jordan block 
\begin_inset Formula $J_{i}$
\end_inset

 is an 
\begin_inset Formula $r_{i}\times r_{i}$
\end_inset

 submatrix in the form 
\begin_inset Formula 
\begin{equation}
J_{i}=\left[\begin{array}{ccccc}
\lambda_{i} & 1\\
 & \lambda_{i} & 1\\
 &  & \lambda_{i} & \ddots\\
 &  &  & \ddots & 1\\
 &  &  &  & \lambda_{i}
\end{array}\right]
\end{equation}

\end_inset

Thus, the matrix 
\begin_inset Formula $J$
\end_inset

 and 
\begin_inset Formula $A$
\end_inset

 are similar and have the same eigenvalues 
\begin_inset Formula $\lambda_{i},i=1,\ldots m$
\end_inset


\end_layout

\begin_layout Proof
A direct computation of the power iteration of matrix 
\begin_inset Formula $A$
\end_inset

 will give us the following equation
\begin_inset Formula 
\[
A^{k}=TJ^{k}T^{-1}=T\left[\begin{array}{cccc}
J_{1}^{k} &  &  & O\\
 & J_{2}^{k}\\
O &  & \ddots & J_{m}^{k}
\end{array}\right]T^{-1}
\]

\end_inset

because the property of Jordan block, the power of each Jordan block will
 have the form
\begin_inset Formula 
\[
J_{i}^{2}=\left[\begin{array}{ccccc}
\lambda_{i}^{2} & 2\lambda_{i} & 1\\
 & \lambda_{i}^{2} & 2\lambda_{i} & \ddots\\
 &  & \lambda_{i}^{2} & \ddots & 1\\
 &  &  & \ddots & 2\lambda_{i}\\
 &  &  &  & \lambda_{i}^{2}
\end{array}\right],\mbox{if }r_{i}\geq3
\]

\end_inset

 and more generally, let 
\begin_inset Formula $J_{i}^{k}=\left[c_{m,n}\left(i,k\right)\right]$
\end_inset

, 
\begin_inset Formula $1\leq m$
\end_inset

, 
\begin_inset Formula $n\leq r_{i}$
\end_inset

, and it has 
\begin_inset Formula 
\[
c_{m,n}\left(i,k\right)=\begin{cases}
0 & n<m\\
\left(\begin{array}{c}
k\\
n-m
\end{array}\right)\lambda_{i}^{k-n+m} & m\leq n\leq\min\left(r_{i},k+m\right)\\
0 & k+m<n<r_{i}
\end{cases}
\]

\end_inset

 Since 
\begin_inset Formula $\rho\left(A\right)<1$
\end_inset

, and matrix 
\begin_inset Formula $J$
\end_inset

 share the same eigenvalues with 
\begin_inset Formula $A$
\end_inset

, 
\begin_inset Formula $\left|\lambda_{i}\right|<1$
\end_inset

.
 This lead to 
\begin_inset Formula $\lim_{k\to\infty}c_{m,n}\left(i,k\right)=0,\:\mbox{for all }1\leq m\leq r_{i},1\leq n\leq r_{i}$
\end_inset

, so that the each Jordan block is convergent.
 Therefore, the matrix iteration 
\begin_inset Formula $A^{k}=TJ^{k}T^{-1}$
\end_inset

 is also convergent.
 This completes the proof.
 
\end_layout

\begin_layout Standard
We give the proof of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:Convergent <=> p(A)<1"

\end_inset

 here as it will be very useful in the proof of an convergence conditions
 theorem for distributed consensus algorithm, see 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sub:DT-First-Order-DAC"

\end_inset

.
 At the same time, the Jordan normal form weight matrix 
\begin_inset Formula $W^{k}=TJ^{k}T^{-1}$
\end_inset

 gives the local value vector 
\begin_inset Formula $\mathbf{x}\left(k\right)=W^{k}\mathbf{x}\left(0\right)$
\end_inset

 another expression in terms of eigenvalues and eigenvectors, which reflects
 the basic ideas of the finite-time consensus algorithm.
 This will be introduced in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:Finite-Time-DAC-Simple"

\end_inset

.
 
\end_layout

\begin_layout Subsubsection
Gerschgorin's theorem
\end_layout

\begin_layout Standard
The calculation of eigenvalues a matrix 
\begin_inset Formula $A$
\end_inset

 involves determination of the matrix 
\begin_inset Formula $\lambda I-A$
\end_inset

 and solving a high order polynomial equation.
 In some situations, for example, when the matrix dimension is very large,
 it is very difficult to determine the spectral radius precisely.
 However, the following theorem of Gerschgorin provides an upper bound of
 the spectral radius 
\begin_inset CommandInset citation
LatexCommand cite
key "Horn1990"

\end_inset

.
\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $A=\left(a_{i,j}\right)$
\end_inset

 be an arbitrary 
\begin_inset Formula $n\times n$
\end_inset

 matrix.
 Denote the 
\begin_inset Formula 
\begin{equation}
d_{i}=\sum_{j=1,j\neq i}^{n}\left|a_{i,j}\right|
\end{equation}

\end_inset

then all the eigenvalues of matrix 
\begin_inset Formula $A$
\end_inset

 are lie in the union of the disks.
\begin_inset Formula 
\begin{equation}
\left|z-a_{i,i}\right|\leq d_{i},\ 1\leq i\leq n.
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The above theorem is well-known, so the proof is omit here.
 But the theorem immediately give the result of
\end_layout

\begin_layout Corollary
\begin_inset CommandInset label
LatexCommand label
name "cor:Upper Bound of p(A)"

\end_inset

Let 
\begin_inset Formula $A=\left(a_{i,j}\right)$
\end_inset

 be an arbitrary 
\begin_inset Formula $n\times n$
\end_inset

 matrix, and 
\begin_inset Formula 
\begin{equation}
v=\max_{1\leq i\leq n}\sum_{j=1}^{n}\left|a_{i,j}\right|
\end{equation}

\end_inset

then we have 
\begin_inset Formula $\rho\left(A\right)\leq v$
\end_inset

.
 
\end_layout

\begin_layout Standard
Therefore, the maximum of the row sums of the modular of the entries of
 matrix 
\begin_inset Formula $A$
\end_inset

 provides a upper bound for spectral radius 
\begin_inset Formula $\rho\left(A\right)$
\end_inset

.
 Since 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $A^{T}$
\end_inset

 have the same eigenvalues, applying the boundary of spectral radius
\begin_inset CommandInset ref
LatexCommand formatted
reference "cor:Upper Bound of p(A)"

\end_inset

 to 
\begin_inset Formula $A^{T}$
\end_inset

 will lead to 
\end_layout

\begin_layout Corollary
Let 
\begin_inset Formula $A=\left(a_{i,j}\right)$
\end_inset

 be an arbitrary 
\begin_inset Formula $n\times n$
\end_inset

 matrix, and 
\begin_inset Formula 
\begin{equation}
v'=\max_{1\leq j\leq n}\sum_{i=1}^{n}\left|a_{i,j}\right|
\end{equation}

\end_inset

then we have 
\begin_inset Formula $\rho\left(A\right)\leq v'$
\end_inset

.
\end_layout

\begin_layout Standard
As both the row sum and column sum of the modular of the entries of matrix
 can provide the upper bound for 
\begin_inset Formula $\rho\left(A\right)$
\end_inset

, the minimum of 
\begin_inset Formula $v$
\end_inset

 and 
\begin_inset Formula $v'$
\end_inset

 gives the better upper bound.
 
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
Just as we assume before, the adjacent matrix 
\begin_inset Formula ${\cal A}$
\end_inset

 of the graph 
\begin_inset Formula ${\cal G}$
\end_inset

 is non-negative, which means all elements are non-negative.
 Then, the induced graph Laplacian matrix will have the following result
 using the Gerschgorin's theorem.
 
\end_layout

\begin_layout Standard

\change_deleted 5862369 1355426470

\series bold
Subtitle Generalize from Gerschgorin's theorem
\change_inserted 5862369 1355425634

\end_layout

\begin_layout Subsubsection
Perron-Frobenius Theorem
\end_layout

\begin_layout Standard
The Perron-Frobenius theorem states that if matrix 
\begin_inset Formula $A$
\end_inset

 is 
\change_deleted 5862369 1355527242
nonnegative
\change_inserted 5862369 1355527242
non-negative
\change_unchanged
 and irreducible which means the digraph of matrix A is strongly connected,
 the spectral radius 
\begin_inset Formula $\rho(A)$
\end_inset

 is equal to a simple eigenvalue of 
\begin_inset Formula $A$
\end_inset

 associated with a positive eigenvector 
\begin_inset CommandInset citation
LatexCommand cite
key "Piziak2007"

\end_inset

.
 The details of Perron-Frobenius is as follows.
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:Perron-Frobenius thm."

\end_inset

 Let 
\begin_inset Formula $A$
\end_inset

 be an 
\begin_inset Formula $n\times n$
\end_inset

 and irreducible matrix with non-negative and real numbers as its entries.
 Then, 
\end_layout

\begin_layout Enumerate
\begin_inset Formula $A$
\end_inset

 has a positive real eigenvalue equal to its spectral radius 
\begin_inset Formula $\rho\left(A\right)$
\end_inset

.
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\rho\left(A\right)$
\end_inset

 is a simple eigenvalue of 
\begin_inset Formula $A$
\end_inset

.
\end_layout

\begin_layout Enumerate
To the eigenvalue 
\begin_inset Formula $\rho\left(A\right)$
\end_inset

, the corresponding eigenvector is positive, i.e.
 
\begin_inset Formula $\mathbf{x}>\mathbf{0}$
\end_inset

.
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\rho\left(A\right)$
\end_inset

 increases if any entry of 
\begin_inset Formula $A$
\end_inset

 increases.
\end_layout

\begin_layout Standard
Recall the problem of finding the bounds for the spectral radius, the Perron-Fro
benius theorem provides the nontrivial lower-bound of 
\begin_inset Formula $\rho\left(A\right)$
\end_inset

.
 In the 
\begin_inset CommandInset ref
LatexCommand formatted
reference "cor:Upper Bound of p(A)"

\end_inset

, the nontrivial upper bound of 
\begin_inset Formula $\rho\left(A\right)$
\end_inset

 is found.
 Together with these two results, we can have a conclusion of the spectral
 radius of a non-negative and irreducible matrix given in the following.
\end_layout

\begin_layout Lemma
If 
\begin_inset Formula $A=\left[a_{i,j}\right]$
\end_inset

 is an 
\begin_inset Formula $n\times n$
\end_inset

 non-negative and irreducible matrix, then either 
\begin_inset Formula 
\begin{equation}
\sum_{j=1}^{n}a_{i,j}=\rho\left(A\right),\mbox{for all }1\leq i\leq n,
\end{equation}

\end_inset

or 
\begin_inset Formula 
\begin{equation}
\min_{1\leq i\leq n}\left(\sum_{j=1}^{n}a_{i,j}\right)<\rho\left(A\right)<\max_{1\leq i\leq n}\left(\sum_{j=1}^{n}a_{i,j}\right).
\end{equation}

\end_inset


\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $A=\left[a_{i,j}\right]$
\end_inset

 be an 
\begin_inset Formula $n\times n$
\end_inset

 non-negative and irreducible matrix, for any 
\begin_inset Formula $\mathbf{x}>\mathbf{0}$
\end_inset

, either
\begin_inset Formula 
\begin{equation}
\min_{1\leq i\leq n}\left(\frac{\sum_{j=1}^{n}a_{i,j}x_{j}}{x_{i}}\right)<\rho\left(A\right)<\max_{1\leq i\leq n}\left(\frac{\sum_{j=1}^{n}a_{i,j}x_{j}}{x_{i}}\right)
\end{equation}

\end_inset

or 
\begin_inset Formula 
\begin{equation}
\frac{\sum_{j=1}^{n}a_{i,j}x_{j}}{x_{i}}=\rho\left(A\right),\ \mbox{for all }1\leq i\leq n.
\end{equation}

\end_inset

Moreover,
\begin_inset Formula 
\begin{equation}
\max_{\mathbf{x}\in P}\min_{1\leq i\leq n}\left(\frac{\sum_{j=1}^{n}a_{i,j}x_{j}}{x_{i}}\right)=\rho\left(A\right)=\min_{\mathbf{x}\in P}\max_{1\leq i\leq n}\left(\frac{\sum_{j=1}^{n}a_{i,j}x_{j}}{x_{i}}\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The equality is valid if we choose the 
\begin_inset Formula $\mathbf{x}$
\end_inset

 equal to the positive eigenvector 
\begin_inset Formula $e>0$
\end_inset

 corresponding to the eigenvalue 
\begin_inset Formula $\rho\left(A\right)$
\end_inset

.
 The method shown above will be applicable, because it provides us both
 the upper bounds and lower bounds for the spectral radius of a non-negative
 and irreducible matrix, by a simple algorithm without calculating the determina
tion of 
\begin_inset Formula $\lambda I-A$
\end_inset

.
 
\change_deleted 5862369 1355425133

\end_layout

\begin_layout Standard

\change_deleted 5862369 1355426478

\series bold
Subtitle Generalized from Perron-Frobenius theorem
\end_layout

\begin_layout Subsection
Diagonalizable matrix & Symmetric matrix
\end_layout

\begin_layout Standard
The Jordan normal form of weight matrix 
\begin_inset Formula $W^{k}=TJ^{k}T^{-1}$
\end_inset

 gives the local value vector 
\begin_inset Formula $\mathbf{x}\left(k\right)=W^{k}\mathbf{x}\left(0\right)$
\end_inset

 an analytical expression in terms of eigenvalues and eigenvectors.
 Moreover, if the matrix 
\begin_inset Formula $W$
\end_inset

 is symmetric, the expressions of 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\mathbf{x}\left(k\right)$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 can be simplified.
 Then, algorithms such as the finite-time consensus, can be implemented
 more easily.
 In addition, under the the assumption of symmetric weight matrices, the
 optimal weight matrix which has the fastest convergence rate can be found
 through a semi-definite problem.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Li2010"

\end_inset

.
 
\end_layout

\begin_layout Definition
A matrix 
\begin_inset Formula $A$
\end_inset

 is diagonalized, if and only if there exist a nonsingular matrix 
\begin_inset Formula $T$
\end_inset

, which reduce 
\begin_inset Formula $A$
\end_inset

 into the form 
\begin_inset Formula 
\[
T^{-1}AT=\mbox{diag}\left(\lambda_{1},\lambda_{2},\ldots,\lambda_{n}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Generally, a matrix are diagonalized by an unitary matrix if and only if
 it is normal.
 Normal matrix 
\begin_inset Formula $A$
\end_inset

 means it satisfies 
\begin_inset Formula $A^{H}A=AA^{H}$
\end_inset

.
 Let 
\begin_inset Formula $\lambda_{1},\lambda_{2},\ldots,\lambda_{n}$
\end_inset

 be the eigenvalues of 
\begin_inset Formula $A$
\end_inset

.
 There exists a 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
unitary matrix 
\begin_inset Formula $U$
\end_inset

, which 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
satisfies
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 
\begin_inset Formula $U^{H}=U^{-1}$
\end_inset

, such that 
\begin_inset Formula $U^{-1}AU=\mbox{diag}\left(\lambda_{1},\lambda_{2},\ldots,\lambda_{n}\right)$
\end_inset

.
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
For real normal matrix 
\begin_inset Formula $A$
\end_inset

, if all of its eigenvalues are real, there exists an orthogonal matrix
 
\begin_inset Formula $P,$
\end_inset

 which has 
\begin_inset Formula $P^{T}=P^{-1}$
\end_inset

, reduces the real matrix 
\begin_inset Formula $A$
\end_inset

 into diagonalized form 
\begin_inset Formula $P^{-1}AP=\mbox{diag}\left(\lambda_{1},\lambda_{2},\ldots,\lambda_{n}\right)$
\end_inset

.
 
\end_layout

\begin_layout Standard
Specifically, any real nonsingular symmetric matrix 
\begin_inset Formula $A\in R^{n\times n}$
\end_inset

 has 
\begin_inset Formula $n$
\end_inset

 linearly independent real eigenvectors.
 Moreover, these eigenvectors can be chosen so that they are orthogonal
 to each other with modular one.
 Thus, the real symmetric matrix can be decomposed by an orthogonal matrix
 
\begin_inset Formula $P$
\end_inset

, i.e.
 
\begin_inset Formula $A=P\Lambda P^{-1}$
\end_inset

, where 
\begin_inset Formula $\Lambda$
\end_inset

 is a diagonal matrix whose entries equal to the eigenvalues of 
\begin_inset Formula $A$
\end_inset

.
 Also the symmetric matrix has all its eigenvalues algebra multiplicity
 equal to the geometric multiplicity, so all the Jordan block have size
 one.
\end_layout

\begin_layout Section
Summary
\end_layout

\begin_layout Standard
As shown in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:Asymptotic-Distributed-Consensus"

\end_inset

, a optimized higher order DAC will have better convergence rate than lower
 order one.
 However, there are negligible improvement for the fourth order DAC compared
 to the third order one.
 The algorithm complexity and computational cost will also increase by introduce
 more order.
 The convergence rates of HO-DAC algorithms are depend on 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
eigenvalues of graph Laplacian matrix 
\begin_inset Formula $\lambda_{i}\left(L\right)$
\end_inset

 which is topology dependent.
 Therefore, the optimizations of HO-DAC larger than second order don't 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
 have unique analytic solution.
 
\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:Finite-Time-DAC-Simple"

\end_inset

, we  show that the finite-time DAC algorithm in an invariant network can
 find the average in finite number of iterations, if 
\begin_inset Formula $\mathbf{x}\left(k\right)$
\end_inset

 can be represented by a linear combination of a new normal basis defined
 by eigenvectors of 
\begin_inset Formula $W$
\end_inset

.
 Furthermore, the weight matrix doesn't need to be convergent and can be
 more easily chosen.
\end_layout

\begin_layout Standard
Some distributed signal processing based on consensus algorithm, such as
 data fusion and decision making, information flooding were introduced in
 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:Consensus-Based-Signal"

\end_inset

.
\end_layout

\end_body
\end_document
